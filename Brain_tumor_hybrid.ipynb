{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7k4DJgRIMXyA",
        "outputId": "04acd9b6-e42b-4016-bfab-c8112a50604d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extraction complete!\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Define paths\n",
        "zip_path = r\"C:\\Users\\Admin\\Desktop\\CerebralFusion\\Brain_Tumor_Dataset\\brain_tumor.zip\"\n",
        "extract_path = r\"C:\\Users\\Admin\\Desktop\\CerebralFusion\\Brain_Tumor_Dataset\"\n",
        "\n",
        "# Extract ZIP file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Extraction complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoeVQVNklQpe",
        "outputId": "5554b1d9-128f-4a3f-c349-b949ccf3d012"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset directory found at: C:\\Users\\Admin\\Desktop\\CerebralFusion\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Define the dataset path\n",
        "dataset_path = r\"C:\\Users\\Admin\\Desktop\\CerebralFusion\"\n",
        "\n",
        "# Check if the path exists\n",
        "if os.path.exists(dataset_path):\n",
        "    print(f\"Dataset directory found at: {dataset_path}\")\n",
        "else:\n",
        "    print(\"Dataset directory not found. Please check the path.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDBB2cOjMZeA",
        "outputId": "8bd88f73-cbf9-43c3-b297-78a13776cc6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 4857 images belonging to 4 classes.\n",
            "Found 855 images belonging to 4 classes.\n",
            "Found 1311 images belonging to 4 classes.\n",
            "Training images: 4857\n",
            "Validation images: 855\n",
            "Testing images: 1311\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Set the local base directory\n",
        "base_dir = r\"C:\\Users\\Admin\\Desktop\\CerebralFusion\\Brain_Tumor_Dataset\"\n",
        "categories = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "\n",
        "# Define paths for training, validation, and testing\n",
        "train_dir = os.path.join(base_dir, 'Training')\n",
        "val_dir = os.path.join(base_dir, 'Validation')\n",
        "test_dir = os.path.join(base_dir, 'Testing')\n",
        "\n",
        "# Ensure required directories exist\n",
        "for category in categories:\n",
        "    os.makedirs(os.path.join(train_dir, category), exist_ok=True)\n",
        "    os.makedirs(os.path.join(val_dir, category), exist_ok=True)\n",
        "    os.makedirs(os.path.join(test_dir, category), exist_ok=True)\n",
        "\n",
        "# Use ImageDataGenerator for loading and augmentation\n",
        "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.15)\n",
        "\n",
        "# Training dataset\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "# Validation dataset\n",
        "val_generator = datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Test dataset\n",
        "test_generator = datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Print dataset statistics\n",
        "print(\"Training images:\", train_generator.samples)\n",
        "print(\"Validation images:\", val_generator.samples)\n",
        "print(\"Testing images:\", test_generator.samples)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TMk3-mX_Ma6Q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from skimage.filters import threshold_otsu\n",
        "from skimage import img_as_ubyte\n",
        "from skimage.color import rgb2gray\n",
        "\n",
        "def preprocess_image(image):\n",
        "    # Ensure image is a NumPy array\n",
        "    if not isinstance(image, np.ndarray):\n",
        "        image = np.array(image)\n",
        "\n",
        "    # Convert to grayscale if image is RGB\n",
        "    if len(image.shape) == 3 and image.shape[-1] == 3:  # RGB check\n",
        "        image = rgb2gray(image)\n",
        "\n",
        "    # Apply OTSU thresholding\n",
        "    thresh = threshold_otsu(image)\n",
        "    binary = image > thresh\n",
        "    binary = img_as_ubyte(binary)\n",
        "\n",
        "    # Resize and normalize image\n",
        "    image = tf.image.resize(image[..., np.newaxis], [224, 224])  # Keep single-channel shape\n",
        "    image = image / 255.0  # Normalize\n",
        "\n",
        "    return image.numpy()  # Convert TensorFlow tensor back to NumPy array\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Kfx0DpVMcuc",
        "outputId": "06413350-38a1-46bb-d85a-de7a3218825e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 5712 images belonging to 4 classes.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Update dataset path\n",
        "base_dir = r\"C:\\Users\\Admin\\Desktop\\CerebralFusion\\Brain_Tumor_Dataset\"\n",
        "train_dir = os.path.join(base_dir, \"Training\")\n",
        "\n",
        "# Augmentation settings\n",
        "datagen_aug = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode=\"nearest\"\n",
        ")\n",
        "\n",
        "# Load augmented training data\n",
        "train_generator_aug = datagen_aug.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode=\"categorical\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ftlCicCAzLD",
        "outputId": "dec7b15c-c02b-4586-e6fd-5966a0eac51f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training images: 5712\n"
          ]
        }
      ],
      "source": [
        "print(\"Training images:\", train_generator_aug.samples)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yX998cS6Me6n",
        "outputId": "4c0a684a-5042-4633-be8d-9ee4a9ff91ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 4857 images belonging to 4 classes.\n",
            "Found 855 images belonging to 4 classes.\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "Could not locate function 'mse'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': 'keras.metrics', 'class_name': 'function', 'config': 'mse', 'registered_name': 'mse'}",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[22], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# ✅ Load existing model if available\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_save_path):\n\u001b[1;32m---> 32\u001b[0m     model_vgg \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_save_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Model loaded from local directory.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\CerebralFusion\\new_venv\\lib\\site-packages\\keras\\src\\saving\\saving_api.py:196\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    190\u001b[0m         filepath,\n\u001b[0;32m    191\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    193\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    194\u001b[0m     )\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_h5_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model_from_hdf5\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    204\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\CerebralFusion\\new_venv\\lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py:155\u001b[0m, in \u001b[0;36mload_model_from_hdf5\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    151\u001b[0m training_config \u001b[38;5;241m=\u001b[39m json_utils\u001b[38;5;241m.\u001b[39mdecode(training_config)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m# Compile model.\u001b[39;00m\n\u001b[0;32m    154\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43msaving_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_args_from_training_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m )\n\u001b[0;32m    159\u001b[0m saving_utils\u001b[38;5;241m.\u001b[39mtry_build_compiled_arguments(model)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# Set optimizer weights.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\CerebralFusion\\new_venv\\lib\\site-packages\\keras\\src\\legacy\\saving\\saving_utils.py:146\u001b[0m, in \u001b[0;36mcompile_args_from_training_config\u001b[1;34m(training_config, custom_objects)\u001b[0m\n\u001b[0;32m    144\u001b[0m loss_config \u001b[38;5;241m=\u001b[39m training_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43m_deserialize_nested_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;66;03m# Ensure backwards compatibility for losses in legacy H5 files\u001b[39;00m\n\u001b[0;32m    148\u001b[0m     loss \u001b[38;5;241m=\u001b[39m _resolve_compile_arguments_compat(loss, loss_config, losses)\n",
            "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\CerebralFusion\\new_venv\\lib\\site-packages\\keras\\src\\legacy\\saving\\saving_utils.py:205\u001b[0m, in \u001b[0;36m_deserialize_nested_config\u001b[1;34m(deserialize_fn, config)\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_single_object(config):\n\u001b[1;32m--> 205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdeserialize_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    208\u001b[0m         k: _deserialize_nested_config(deserialize_fn, v)\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    210\u001b[0m     }\n",
            "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\CerebralFusion\\new_venv\\lib\\site-packages\\keras\\src\\losses\\__init__.py:155\u001b[0m, in \u001b[0;36mdeserialize\u001b[1;34m(name, custom_objects)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.losses.deserialize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeserialize\u001b[39m(name, custom_objects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    144\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserializes a serialized loss class/function instance.\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \n\u001b[0;32m    146\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;124;03m        A Keras `Loss` instance or a loss function.\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mserialization_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mALL_OBJECTS_DICT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\CerebralFusion\\new_venv\\lib\\site-packages\\keras\\src\\saving\\serialization_lib.py:575\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    573\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m config\n\u001b[0;32m    574\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module_objects[config], types\u001b[38;5;241m.\u001b[39mFunctionType):\n\u001b[1;32m--> 575\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    576\u001b[0m \u001b[43m                \u001b[49m\u001b[43mserialize_with_public_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    577\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn_module_name\u001b[49m\n\u001b[0;32m    578\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    580\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    581\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m deserialize_keras_object(\n\u001b[0;32m    582\u001b[0m             serialize_with_public_class(\n\u001b[0;32m    583\u001b[0m                 module_objects[config], inner_config\u001b[38;5;241m=\u001b[39minner_config\n\u001b[0;32m    584\u001b[0m             ),\n\u001b[0;32m    585\u001b[0m             custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    586\u001b[0m         )\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PLAIN_TYPES):\n",
            "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\CerebralFusion\\new_venv\\lib\\site-packages\\keras\\src\\saving\\serialization_lib.py:678\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m class_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    677\u001b[0m     fn_name \u001b[38;5;241m=\u001b[39m inner_config\n\u001b[1;32m--> 678\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_retrieve_class_or_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfn_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mregistered_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfull_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;66;03m# Below, handling of all classes.\u001b[39;00m\n\u001b[0;32m    688\u001b[0m \u001b[38;5;66;03m# First, is it a shared object?\u001b[39;00m\n\u001b[0;32m    689\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshared_object_id\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config:\n",
            "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\CerebralFusion\\new_venv\\lib\\site-packages\\keras\\src\\saving\\serialization_lib.py:810\u001b[0m, in \u001b[0;36m_retrieve_class_or_fn\u001b[1;34m(name, registered_name, module, obj_type, full_config, custom_objects)\u001b[0m\n\u001b[0;32m    803\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n\u001b[0;32m    804\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    805\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not deserialize \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m because \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    806\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mits parent module \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cannot be imported. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    807\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull object config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_config\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    808\u001b[0m             )\n\u001b[1;32m--> 810\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    811\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    812\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure custom classes are decorated with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`@keras.saving.register_keras_serializable()`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    814\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull object config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_config\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    815\u001b[0m )\n",
            "\u001b[1;31mTypeError\u001b[0m: Could not locate function 'mse'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': 'keras.metrics', 'class_name': 'function', 'config': 'mse', 'registered_name': 'mse'}"
          ]
        }
      ],
      "source": [
        "# from tensorflow.keras.applications import VGG16\n",
        "# from tensorflow.keras.models import Model, load_model\n",
        "# from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "# import os\n",
        "\n",
        "\n",
        "# # ✅ Define local paths\n",
        "# base_dir = r\"C:\\Users\\Admin\\Desktop\\CerebralFusion\\Brain_Tumor_Dataset\"\n",
        "# train_dir = os.path.join(base_dir, \"Training\")\n",
        "# val_dir = os.path.join(base_dir, \"Validation\")\n",
        "\n",
        "# # ✅ Define model save path\n",
        "# model_save_path = r\"C:\\Users\\Admin\\Desktop\\CerebralFusion\\vgg16_model.h5\"\n",
        "\n",
        "# # ✅ Ensure categories list is defined\n",
        "# categories = [\"glioma\", \"meningioma\", \"notumor\", \"pituitary\"]\n",
        "\n",
        "# # ✅ Data Augmentation\n",
        "# datagen = ImageDataGenerator(rescale=1.0 / 255, validation_split=0.15)\n",
        "\n",
        "# train_generator = datagen.flow_from_directory(\n",
        "#     train_dir, target_size=(224, 224), batch_size=32, class_mode=\"categorical\", subset=\"training\"\n",
        "# )\n",
        "\n",
        "# val_generator = datagen.flow_from_directory(\n",
        "#     train_dir, target_size=(224, 224), batch_size=32, class_mode=\"categorical\", subset=\"validation\"\n",
        "# )\n",
        "\n",
        "# # ✅ Load existing model if available\n",
        "# if os.path.exists(model_save_path):\n",
        "#     model_vgg = load_model(model_save_path)\n",
        "#     print(\"✅ Model loaded from local directory.\")\n",
        "# else:\n",
        "#     print(\"⚠️ No saved model found. Training a new model...\")\n",
        "\n",
        "#     # ✅ Load VGG16 without the top classification layer\n",
        "#     base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "#     # ✅ Add custom layers on top\n",
        "#     x = base_model.output\n",
        "#     x = GlobalAveragePooling2D()(x)  # Reduce parameters, prevent overfitting\n",
        "#     x = Dense(1024, activation=\"relu\")(x)\n",
        "#     predictions = Dense(len(categories), activation=\"softmax\")(x)  # Match number of classes\n",
        "\n",
        "#     # ✅ Define the full model\n",
        "#     model_vgg = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "#     # ✅ Freeze base model layers (transfer learning)\n",
        "#     for layer in base_model.layers:\n",
        "#         layer.trainable = False\n",
        "\n",
        "#     # ✅ Compile the model\n",
        "#     model_vgg.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "#     # ✅ Train the model for 20 epochs\n",
        "#     history_vgg = model_vgg.fit(train_generator, validation_data=val_generator, epochs=20)\n",
        "\n",
        "#     # ✅ Save the trained model locally\n",
        "#     model_vgg.save(model_save_path)\n",
        "#     print(\"✅ Model trained and saved locally.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 4857 images belonging to 4 classes.\n",
            "Found 855 images belonging to 4 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ VGG19 model loaded from local directory.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.applications import VGG19 \n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "\n",
        "# Define local paths\n",
        "base_dir = r\"C:\\Users\\Admin\\Desktop\\CerebralFusion\\Brain_Tumor_Dataset\"\n",
        "train_dir = os.path.join(base_dir, \"Training\")\n",
        "val_dir = os.path.join(base_dir, \"Validation\")\n",
        "\n",
        "# Define model save path (changed to vgg19)\n",
        "model_save_path = r\"C:\\Users\\Admin\\Desktop\\CerebralFusion\\vgg19_model.h5\"  # Changed filename\n",
        "\n",
        "# Ensure categories list is defined\n",
        "categories = [\"glioma\", \"meningioma\", \"notumor\", \"pituitary\"]\n",
        "\n",
        "# Data Augmentation\n",
        "datagen = ImageDataGenerator(rescale=1.0 / 255, validation_split=0.15)\n",
        "\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    train_dir, target_size=(224, 224), batch_size=32, class_mode=\"categorical\", subset=\"training\"\n",
        ")\n",
        "\n",
        "val_generator = datagen.flow_from_directory(\n",
        "    train_dir, target_size=(224, 224), batch_size=32, class_mode=\"categorical\", subset=\"validation\"\n",
        ")\n",
        "\n",
        "# Load existing model if available\n",
        "if os.path.exists(model_save_path):\n",
        "    model_vgg = load_model(model_save_path)\n",
        "    print(\"✅ VGG19 model loaded from local directory.\")\n",
        "else:\n",
        "    print(\"⚠️ No saved model found. Training a new VGG19 model...\")\n",
        "\n",
        "    # Load VGG19 without the top classification layer \n",
        "    base_model = VGG19(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "    # Add custom layers on top\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)  # Reduce parameters, prevent overfitting\n",
        "    x = Dense(1024, activation=\"relu\")(x)\n",
        "    predictions = Dense(len(categories), activation=\"softmax\")(x)  # Match number of classes\n",
        "\n",
        "    # Define the full model\n",
        "    model_vgg = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "    # Freeze base model layers (transfer learning)\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Compile the model\n",
        "    model_vgg.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    # Train the model for 20 epochs\n",
        "    history_vgg = model_vgg.fit(\n",
        "        train_generator, \n",
        "        validation_data=val_generator, \n",
        "        epochs=20,\n",
        "        verbose=1  # Show progress bar\n",
        "    )\n",
        "\n",
        "    # Save the trained model locally\n",
        "    model_vgg.save(model_save_path)\n",
        "    print(f\"✅ VGG19 model trained and saved at {model_save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nw3ZfthPMhTm",
        "outputId": "1eb2ef87-db3a-400f-9db7-f972b277d89e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 4857 images belonging to 4 classes.\n",
            "Found 855 images belonging to 4 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ ResNet50 model loaded from local directory.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "\n",
        "# ✅ Define local paths\n",
        "base_dir = r\"C:\\Users\\Admin\\Desktop\\CerebralFusion\\Brain_Tumor_Dataset\"\n",
        "train_dir = os.path.join(base_dir, \"Training\")\n",
        "val_dir = os.path.join(base_dir, \"Validation\")\n",
        "\n",
        "# ✅ Define model save path\n",
        "model_save_path = r\"C:\\Users\\Admin\\Desktop\\CerebralFusion\\resnet50_model.h5\"\n",
        "\n",
        "# ✅ Ensure categories list is defined\n",
        "categories = [\"glioma\", \"meningioma\", \"notumor\", \"pituitary\"]\n",
        "\n",
        "# ✅ Data Augmentation\n",
        "datagen = ImageDataGenerator(rescale=1.0 / 255, validation_split=0.15)\n",
        "\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    train_dir, target_size=(224, 224), batch_size=32, class_mode=\"categorical\", subset=\"training\"\n",
        ")\n",
        "\n",
        "val_generator = datagen.flow_from_directory(\n",
        "    train_dir, target_size=(224, 224), batch_size=32, class_mode=\"categorical\", subset=\"validation\"\n",
        ")\n",
        "\n",
        "# ✅ Load existing model if available\n",
        "if os.path.exists(model_save_path):\n",
        "    model_resnet = load_model(model_save_path)\n",
        "    print(\"✅ ResNet50 model loaded from local directory.\")\n",
        "else:\n",
        "    print(\"⚠️ No saved model found. Training a new ResNet50 model...\")\n",
        "\n",
        "    # ✅ Load ResNet50 without the top classification layer\n",
        "    base_model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "    # ✅ Add custom layers on top\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)  # Reduce parameters, prevent overfitting\n",
        "    x = Dense(1024, activation=\"relu\")(x)\n",
        "    predictions = Dense(len(categories), activation=\"softmax\")(x)  # Match number of classes\n",
        "\n",
        "    # ✅ Define the full model\n",
        "    model_resnet = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "    # ✅ Freeze base model layers (transfer learning)\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # ✅ Compile the model\n",
        "    model_resnet.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    # ✅ Train the model for 20 epochs\n",
        "    history_resnet = model_resnet.fit(train_generator, validation_data=val_generator, epochs=20)\n",
        "\n",
        "    # ✅ Save the trained model locally\n",
        "    model_resnet.save(model_save_path)\n",
        "    print(\"✅ ResNet50 model trained and saved locally.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFOBoyJaMi6y",
        "outputId": "3631eeaf-a476-4068-dfa8-67093e329f00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 4857 images belonging to 4 classes.\n",
            "Found 855 images belonging to 4 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ DenseNet121 model loaded from local directory.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.applications import DenseNet121\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "\n",
        "# ✅ Define local paths\n",
        "base_dir = r\"C:\\Users\\Admin\\Desktop\\CerebralFusion\\Brain_Tumor_Dataset\"\n",
        "train_dir = os.path.join(base_dir, \"Training\")\n",
        "val_dir = os.path.join(base_dir, \"Validation\")\n",
        "\n",
        "# ✅ Define model save path\n",
        "model_save_path = r\"C:\\Users\\Admin\\Desktop\\CerebralFusion\\densenet121_model.h5\"\n",
        "\n",
        "# ✅ Ensure categories list is defined\n",
        "categories = [\"glioma\", \"meningioma\", \"notumor\", \"pituitary\"]\n",
        "\n",
        "# ✅ Data Augmentation\n",
        "datagen = ImageDataGenerator(rescale=1.0 / 255, validation_split=0.15)\n",
        "\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    train_dir, target_size=(224, 224), batch_size=32, class_mode=\"categorical\", subset=\"training\"\n",
        ")\n",
        "\n",
        "val_generator = datagen.flow_from_directory(\n",
        "    train_dir, target_size=(224, 224), batch_size=32, class_mode=\"categorical\", subset=\"validation\"\n",
        ")\n",
        "\n",
        "# ✅ Load existing model if available\n",
        "if os.path.exists(model_save_path):\n",
        "    model_densenet = load_model(model_save_path)\n",
        "    print(\"✅ DenseNet121 model loaded from local directory.\")\n",
        "else:\n",
        "    print(\"⚠️ No saved model found. Training a new DenseNet121 model...\")\n",
        "\n",
        "    # ✅ Load DenseNet121 without the top classification layer\n",
        "    base_model = DenseNet121(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "    # ✅ Add custom layers on top\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)  # Reduce parameters, prevent overfitting\n",
        "    x = Dense(1024, activation=\"relu\")(x)\n",
        "    predictions = Dense(len(categories), activation=\"softmax\")(x)  # Match number of classes\n",
        "\n",
        "    # ✅ Define the full model\n",
        "    model_densenet = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "    # ✅ Freeze base model layers (transfer learning)\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # ✅ Compile the model\n",
        "    model_densenet.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    # ✅ Train the model for 20 epochs\n",
        "    history_densenet = model_densenet.fit(train_generator, validation_data=val_generator, epochs=20)\n",
        "\n",
        "    # ✅ Save the trained model locally\n",
        "    model_densenet.save(model_save_path)\n",
        "    print(\"✅ DenseNet121 model trained and saved locally.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qe64aBnqM0uu",
        "outputId": "0c229746-bb7a-482b-80c8-56d7a57db000"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 4857 images belonging to 4 classes.\n",
            "Found 855 images belonging to 4 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Custom CNN model loaded from local directory.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "\n",
        "# ✅ Define local paths\n",
        "base_dir = r\"C:\\Users\\Admin\\Desktop\\CerebralFusion\\Brain_Tumor_Dataset\"\n",
        "train_dir = os.path.join(base_dir, \"Training\")\n",
        "val_dir = os.path.join(base_dir, \"Validation\")\n",
        "\n",
        "# ✅ Define model save path\n",
        "model_save_path = r\"C:\\Users\\Admin\\Desktop\\CerebralFusion\\custom_cnn_model.h5\"\n",
        "\n",
        "# ✅ Ensure categories list is defined\n",
        "categories = [\"glioma\", \"meningioma\", \"notumor\", \"pituitary\"]\n",
        "\n",
        "# ✅ Data Augmentation\n",
        "datagen = ImageDataGenerator(rescale=1.0 / 255, validation_split=0.15)\n",
        "\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    train_dir, target_size=(224, 224), batch_size=32, class_mode=\"categorical\", subset=\"training\"\n",
        ")\n",
        "\n",
        "val_generator = datagen.flow_from_directory(\n",
        "    train_dir, target_size=(224, 224), batch_size=32, class_mode=\"categorical\", subset=\"validation\"\n",
        ")\n",
        "\n",
        "# ✅ Load existing model if available\n",
        "if os.path.exists(model_save_path):\n",
        "    model_custom = load_model(model_save_path)\n",
        "    print(\"✅ Custom CNN model loaded from local directory.\")\n",
        "else:\n",
        "    print(\"⚠️ No saved model found. Training a new Custom CNN model...\")\n",
        "\n",
        "    # ✅ Build the Custom CNN model\n",
        "    model_custom = Sequential([\n",
        "        Conv2D(32, (3, 3), activation=\"relu\", input_shape=(224, 224, 3)),  \n",
        "        MaxPooling2D((2, 2)),  \n",
        "        Conv2D(64, (3, 3), activation=\"relu\"),  \n",
        "        MaxPooling2D((2, 2)),  \n",
        "        Conv2D(128, (3, 3), activation=\"relu\"),  \n",
        "        MaxPooling2D((2, 2)),  \n",
        "        Flatten(),  \n",
        "        Dense(128, activation=\"relu\"),  \n",
        "        Dense(len(categories), activation=\"softmax\")  \n",
        "    ])\n",
        "\n",
        "    # ✅ Compile the model\n",
        "    model_custom.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    # ✅ Train the model for 20 epochs\n",
        "    history_custom = model_custom.fit(train_generator, validation_data=val_generator, epochs=20)\n",
        "\n",
        "    # ✅ Save the trained model locally\n",
        "    model_custom.save(model_save_path)\n",
        "    print(\"✅ Custom CNN model trained and saved locally.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_PgQN-69_G1",
        "outputId": "d7dd3b06-70ca-4772-826a-cdbd4a29a636"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ SVM model loaded from local directory.\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import joblib\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# ✅ Define local paths\n",
        "base_dir = r\"C:\\Users\\Admin\\Desktop\\CerebralFusion\\Brain_Tumor_Dataset\"\n",
        "train_dir = os.path.join(base_dir, \"Training\")\n",
        "\n",
        "# ✅ Define model save path\n",
        "model_save_path = r\"C:\\Users\\Admin\\Desktop\\CerebralFusion\\svm_model.pkl\"\n",
        "\n",
        "# ✅ Ensure categories list is defined\n",
        "categories = [\"glioma\", \"meningioma\", \"notumor\", \"pituitary\"]\n",
        "\n",
        "# ✅ Function to extract Gabor features\n",
        "def extract_gabor_features(images):\n",
        "    features = []\n",
        "    kernels = []\n",
        "    \n",
        "    for theta in range(4):\n",
        "        theta = theta / 4. * np.pi\n",
        "        for sigma in (1, 3):\n",
        "            for lamda in np.arange(0, np.pi, np.pi / 4):\n",
        "                for gamma in (0.05, 0.5):\n",
        "                    kernel = cv2.getGaborKernel((21, 21), sigma, theta, lamda, gamma, 0, ktype=cv2.CV_32F)\n",
        "                    kernels.append(kernel)\n",
        "\n",
        "    for image in images:\n",
        "        feature_vector = []\n",
        "        for kernel in kernels:\n",
        "            filtered = cv2.filter2D(image, cv2.CV_8UC3, kernel)\n",
        "            feature_vector.append(filtered.mean())\n",
        "        features.append(feature_vector)\n",
        "\n",
        "    return np.array(features)\n",
        "\n",
        "# ✅ Check if model exists\n",
        "if os.path.exists(model_save_path):\n",
        "    svm_model = joblib.load(model_save_path)\n",
        "    print(\"✅ SVM model loaded from local directory.\")\n",
        "else:\n",
        "    print(\"⚠️ No saved model found. Training a new SVM model...\")\n",
        "\n",
        "    # ✅ Load images and labels\n",
        "    images, labels = [], []\n",
        "    \n",
        "    for category in categories:\n",
        "        category_dir = os.path.join(train_dir, category)\n",
        "        \n",
        "        for img_name in os.listdir(category_dir):\n",
        "            img_path = os.path.join(category_dir, img_name)\n",
        "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "            img = cv2.resize(img, (224, 224))\n",
        "            images.append(img)\n",
        "            labels.append(categories.index(category))\n",
        "\n",
        "    # ✅ Extract Gabor features\n",
        "    X = extract_gabor_features(images)\n",
        "    y = np.array(labels)\n",
        "\n",
        "    # ✅ Split data into training and validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # ✅ Train SVM classifier\n",
        "    svm_model = SVC(kernel=\"linear\", probability=True)\n",
        "    svm_model.fit(X_train, y_train)\n",
        "\n",
        "    # ✅ Save the trained model locally\n",
        "    joblib.dump(svm_model, model_save_path)\n",
        "    print(\"✅ SVM model trained and saved locally.\")\n",
        "\n",
        "# ✅ Evaluate SVM model\n",
        "# y_pred = svm_model.predict(X_val)\n",
        "# print(f\"✅ SVM Accuracy: {accuracy_score(y_val, y_pred):.4f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2o_aGNT59-03",
        "outputId": "17949bd1-1b92-4616-dbef-b7304521deb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Random Forest model loaded from local directory.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from skimage.feature import hog\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# ✅ Define local paths\n",
        "base_dir = r\"C:\\Users\\Admin\\Desktop\\CerebralFusion\\Brain_Tumor_Dataset\"\n",
        "train_dir = os.path.join(base_dir, \"Training\")\n",
        "\n",
        "# ✅ Define model save path\n",
        "model_save_path = r\"C:\\Users\\Admin\\Desktop\\CerebralFusion\\random_forest_model.pkl\"\n",
        "\n",
        "# ✅ Ensure categories list is defined\n",
        "categories = [\"glioma\", \"meningioma\", \"notumor\", \"pituitary\"]\n",
        "\n",
        "# ✅ Function to extract HOG features\n",
        "def extract_hog_features(images):\n",
        "    features = []\n",
        "    \n",
        "    for image in images:\n",
        "        # Convert RGB to grayscale if necessary\n",
        "        if len(image.shape) == 3 and image.shape[2] == 3:\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "        # Extract HOG features\n",
        "        fd = hog(image, pixels_per_cell=(8, 8), cells_per_block=(2, 2), \n",
        "                 visualize=False, channel_axis=None)\n",
        "        features.append(fd)\n",
        "    \n",
        "    return np.array(features)\n",
        "\n",
        "# ✅ Check if model exists\n",
        "if os.path.exists(model_save_path):\n",
        "    rf_model = joblib.load(model_save_path)\n",
        "    print(\"✅ Random Forest model loaded from local directory.\")\n",
        "else:\n",
        "    print(\"⚠️ No saved model found. Training a new Random Forest model...\")\n",
        "\n",
        "    # ✅ Load images and labels\n",
        "    images, labels = [], []\n",
        "    \n",
        "    for category in categories:\n",
        "        category_dir = os.path.join(train_dir, category)\n",
        "        \n",
        "        for img_name in os.listdir(category_dir):\n",
        "            img_path = os.path.join(category_dir, img_name)\n",
        "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "            img = cv2.resize(img, (224, 224))\n",
        "            images.append(img)\n",
        "            labels.append(categories.index(category))\n",
        "\n",
        "    # ✅ Extract HOG features\n",
        "    X_hog = extract_hog_features(images)\n",
        "    y = np.array(labels)\n",
        "\n",
        "    # ✅ Split data into training and validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_hog, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # ✅ Train Random Forest model\n",
        "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "\n",
        "    # ✅ Save the trained model locally\n",
        "    joblib.dump(rf_model, model_save_path)\n",
        "    print(\"✅ Random Forest model trained and saved locally.\")\n",
        "\n",
        "# ✅ Evaluate Random Forest model\n",
        "# y_pred = rf_model.predict(X_val)\n",
        "# print(f\"✅ Random Forest Accuracy: {accuracy_score(y_val, y_pred):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-WriyTuKLNq",
        "outputId": "abc75bc4-3231-4a26-ba26-d594aea17e9c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 791ms/step\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 546ms/step\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 719ms/step\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step\n",
            "Model Performance:\n",
            "              Accuracy  F1-Score   AUC-ROC\n",
            "VGG19         0.859649  0.857641  0.966334\n",
            "ResNet50      0.742690  0.724384  0.910129\n",
            "DenseNet      0.921637  0.921792  0.990932\n",
            "CNN           0.826901  0.829412  0.947507\n",
            "SVM           0.231579  0.087090  0.687983\n",
            "RandomForest  0.923977  0.923955  0.990631\n",
            "\n",
            "Top 3 Models: ['RandomForest', 'DenseNet', 'VGG19']\n"
          ]
        }
      ],
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import cv2\n",
        "# from tensorflow.keras.models import load_model\n",
        "# from joblib import load  # Use joblib for .pkl models\n",
        "# from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "# # Load deep learning models\n",
        "# model_vgg = load_model(\"vgg19_model.h5\")\n",
        "# model_vgg.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# model_resnet = load_model(\"resnet50_model.h5\")\n",
        "# model_resnet.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# model_densenet = load_model(\"densenet121_model.h5\")\n",
        "# model_densenet.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# model_custom = load_model(\"custom_cnn_model.h5\")\n",
        "# model_custom.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# # Load machine learning models using joblib\n",
        "# svm_model = load(\"svm_model.pkl\")\n",
        "# rf_model = load(\"random_forest_model.pkl\")\n",
        "\n",
        "# # Function to evaluate a model\n",
        "# def evaluate_model(model, X_val, y_val, is_deep_learning=False):\n",
        "#     if is_deep_learning:\n",
        "#         y_pred_proba = model.predict(X_val)\n",
        "#         y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "#         y_val = np.argmax(y_val, axis=1)  # Convert one-hot to integer labels\n",
        "#     else:\n",
        "#         y_pred = model.predict(X_val)\n",
        "#         y_pred_proba = model.predict_proba(X_val)\n",
        "\n",
        "#     accuracy = accuracy_score(y_val, y_pred)\n",
        "#     f1 = f1_score(y_val, y_pred, average='weighted')\n",
        "\n",
        "#     # Handle AUC-ROC correctly for multi-class classification\n",
        "#     if len(np.unique(y_val)) > 2:  # Multi-class case\n",
        "#         auc_roc = roc_auc_score(y_val, y_pred_proba, multi_class='ovr')\n",
        "#     else:  # Binary case\n",
        "#         auc_roc = roc_auc_score(y_val, y_pred_proba[:, 1])\n",
        "\n",
        "#     return accuracy, f1, auc_roc\n",
        "\n",
        "# # Dictionary to store models\n",
        "# models = {\n",
        "#     'VGG19': model_vgg,\n",
        "#     'ResNet50': model_resnet,\n",
        "#     'DenseNet': model_densenet,\n",
        "#     'CNN': model_custom,\n",
        "#     'SVM': svm_model,\n",
        "#     'RandomForest': rf_model\n",
        "# }\n",
        "\n",
        "# # Get the full validation data\n",
        "# X_val_dl_full = []\n",
        "# y_val_dl_full = []\n",
        "# for i in range(len(val_generator)):\n",
        "#     X_batch, y_batch = val_generator[i]\n",
        "#     X_val_dl_full.append(X_batch)\n",
        "#     y_val_dl_full.append(y_batch)\n",
        "# X_val_dl_full = np.vstack(X_val_dl_full)\n",
        "# y_val_dl_full = np.vstack(y_val_dl_full)\n",
        "\n",
        "# # Convert images to grayscale for HOG feature extraction\n",
        "# X_val_dl_gray = np.array([cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) if len(img.shape) == 3 else img for img in X_val_dl_full])\n",
        "\n",
        "# # Extract features for SVM and Random Forest\n",
        "# X_val_gabor = extract_gabor_features(X_val_dl_full)  # Gabor features for SVM\n",
        "# X_val_hog = extract_hog_features(X_val_dl_gray)  # HOG features for Random Forest\n",
        "\n",
        "# # Evaluate models\n",
        "# results = {}\n",
        "# for name, model in models.items():\n",
        "#     if name in ['VGG19', 'ResNet50', 'DenseNet', 'CNN']:\n",
        "#         accuracy, f1, auc_roc = evaluate_model(model, X_val_dl_full, y_val_dl_full, is_deep_learning=True)\n",
        "#     else:\n",
        "#         X_val_fe = X_val_gabor if name == 'SVM' else X_val_hog\n",
        "#         accuracy, f1, auc_roc = evaluate_model(model, X_val_fe, np.argmax(y_val_dl_full, axis=1), is_deep_learning=False)\n",
        "\n",
        "#     results[name] = {'Accuracy': accuracy, 'F1-Score': f1, 'AUC-ROC': auc_roc}\n",
        "\n",
        "# # Convert results to DataFrame\n",
        "# results_df = pd.DataFrame(results).T\n",
        "# print(\"Model Performance:\")\n",
        "# print(results_df)\n",
        "\n",
        "# # Select top 3 models\n",
        "# results_df['Combined_Score'] = (results_df['Accuracy'] + results_df['F1-Score'] + results_df['AUC-ROC']) / 3\n",
        "# top_3_models = results_df.nlargest(3, 'Combined_Score').index.tolist()\n",
        "# print(\"\\nTop 3 Models:\", top_3_models)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded VGG19 successfully\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded ResNet50 successfully\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded DenseNet successfully\n",
            "Loaded CNN successfully\n",
            "Loaded SVM successfully\n",
            "Loaded RandomForest successfully\n",
            "\n",
            "Evaluating VGG19...\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 768ms/step\n",
            "VGG19: Accuracy = 0.8848, F1-Score = 0.8829, AUC-ROC = 0.9814\n",
            "\n",
            "Evaluating ResNet50...\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 516ms/step\n",
            "ResNet50: Accuracy = 0.7689, F1-Score = 0.7485, AUC-ROC = 0.9355\n",
            "\n",
            "Evaluating DenseNet...\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 650ms/step\n",
            "DenseNet: Accuracy = 0.9481, F1-Score = 0.9478, AUC-ROC = 0.9943\n",
            "\n",
            "Evaluating CNN...\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step\n",
            "CNN: Accuracy = 0.9603, F1-Score = 0.9600, AUC-ROC = 0.9927\n",
            "\n",
            "Evaluating SVM...\n",
            "SVM: Accuracy = 0.2288, F1-Score = 0.0852, AUC-ROC = 0.6468\n",
            "\n",
            "Evaluating RandomForest...\n",
            "RandomForest: Accuracy = 0.8940, F1-Score = 0.8924, AUC-ROC = 0.9805\n",
            "\n",
            "Model Performance on Test Data:\n",
            "              Accuracy  F1-Score   AUC-ROC\n",
            "VGG19         0.884821  0.882901  0.981351\n",
            "ResNet50      0.768879  0.748500  0.935512\n",
            "DenseNet      0.948131  0.947838  0.994280\n",
            "CNN           0.960336  0.960037  0.992660\n",
            "SVM           0.228833  0.085226  0.646784\n",
            "RandomForest  0.893974  0.892372  0.980507\n",
            "\n",
            "Top 3 Models on Test Data: ['CNN', 'DenseNet', 'RandomForest']\n"
          ]
        }
      ],
      "source": [
        "# top 3 models on test data\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from tensorflow.keras.models import load_model\n",
        "from joblib import load  # Use joblib for .pkl models\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "import os\n",
        "from skimage.feature import hog\n",
        "\n",
        "# Configuration\n",
        "model_mapping = {\n",
        "    'VGG19': 'vgg19_model.h5',\n",
        "    'ResNet50': 'resnet50_model.h5',\n",
        "    'DenseNet': 'densenet121_model.h5',\n",
        "    'CNN': 'custom_cnn_model.h5',\n",
        "    'SVM': 'svm_model.pkl',\n",
        "    'RandomForest': 'random_forest_model.pkl'\n",
        "}\n",
        "\n",
        "class_labels = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "test_dir = r'C:\\Users\\Admin\\Desktop\\CerebralFusion\\Brain_Tumor_Dataset\\Testing'\n",
        "model_dir = '.' # Assuming your model files are in the current directory, adjust if needed\n",
        "\n",
        "# Load models\n",
        "models = {}\n",
        "for name, file in model_mapping.items():\n",
        "    path = os.path.join(model_dir, file)\n",
        "    try:\n",
        "        if file.endswith('.h5'):\n",
        "            models[name] = load_model(path)\n",
        "        else:\n",
        "            models[name] = load(path)\n",
        "        print(f\"Loaded {name} successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {name}: {e}\")\n",
        "\n",
        "# Load test data\n",
        "def load_test_data(test_dir, target_size=(224, 224)):\n",
        "    X_test = []\n",
        "    y_test = []\n",
        "    for i, label in enumerate(class_labels):\n",
        "        label_dir = os.path.join(test_dir, label)\n",
        "        if not os.path.exists(label_dir):\n",
        "            continue\n",
        "        for img_file in os.listdir(label_dir):\n",
        "            img_path = os.path.join(label_dir, img_file)\n",
        "            try:\n",
        "                img = cv2.imread(img_path)\n",
        "                if img is not None:\n",
        "                    img_resized = cv2.resize(img, target_size)\n",
        "                    img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n",
        "                    img_normalized = img_rgb.astype('float32') / 255.0\n",
        "                    X_test.append(img_normalized)\n",
        "                    y_test.append(i)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {img_path}: {e}\")\n",
        "    return np.array(X_test), np.array(y_test)\n",
        "\n",
        "X_test_dl, y_test_int = load_test_data(test_dir)\n",
        "y_test_onehot = pd.get_dummies(y_test_int).values\n",
        "\n",
        "# Convert images to grayscale for HOG feature extraction\n",
        "X_test_gray = np.array([cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) if len(img.shape) == 3 else img for img in X_test_dl])\n",
        "\n",
        "# Function to extract HOG features (ensure this is defined as in your first code)\n",
        "def extract_hog_features(image):\n",
        "    if len(image.shape) > 2:\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "    if image.dtype != np.uint8:\n",
        "        image = (image * 255).astype(np.uint8)\n",
        "    features = hog(\n",
        "        image,\n",
        "        orientations=9,\n",
        "        pixels_per_cell=(8, 8),\n",
        "        cells_per_block=(2, 2),\n",
        "        block_norm='L2-Hys',\n",
        "        visualize=False,\n",
        "        feature_vector=True\n",
        "    )\n",
        "    return features\n",
        "\n",
        "# Extract features for SVM and Random Forest (assuming extract_gabor_features is defined elsewhere)\n",
        "def extract_gabor_features(images):\n",
        "    features = []\n",
        "    kernels = []\n",
        "    for theta in range(4):\n",
        "        theta = theta / 4. * np.pi\n",
        "        for sigma in (1, 3):\n",
        "            for lamda in np.arange(0, np.pi, np.pi / 4):\n",
        "                for gamma in (0.05, 0.5):\n",
        "                    kernel = cv2.getGaborKernel((21, 21), sigma, theta, lamda, gamma, 0, ktype=cv2.CV_32F)\n",
        "                    kernels.append(kernel)\n",
        "    for image in images:\n",
        "        feature_vector = []\n",
        "        for kernel in kernels:\n",
        "            filtered = cv2.filter2D(image, cv2.CV_8UC3, kernel)\n",
        "            feature_vector.append(filtered.mean())\n",
        "        features.append(feature_vector)\n",
        "    return np.array(features)\n",
        "\n",
        "X_test_gabor = extract_gabor_features(X_test_dl)\n",
        "X_test_hog = np.array([extract_hog_features(img) for img in X_test_gray])\n",
        "\n",
        "# Function to evaluate a model\n",
        "def evaluate_model(model, X_test, y_test, is_deep_learning=False):\n",
        "    if is_deep_learning:\n",
        "        y_pred_proba = model.predict(X_test)\n",
        "        y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "        y_true = np.argmax(y_test, axis=1)  # Convert one-hot to integer labels\n",
        "    else:\n",
        "        y_pred = model.predict(X_test)\n",
        "        try:\n",
        "            y_pred_proba = model.predict_proba(X_test)\n",
        "        except AttributeError:\n",
        "            # Some classifiers might not have predict_proba (e.g., linear SVM without probability=True)\n",
        "            y_pred_proba = None\n",
        "        y_true = y_test\n",
        "\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "    auc_roc = np.nan\n",
        "    if y_pred_proba is not None:\n",
        "        if len(np.unique(y_true)) > 2:  # Multi-class case\n",
        "            try:\n",
        "                auc_roc = roc_auc_score(y_true, y_pred_proba, multi_class='ovr', labels=np.unique(y_true))\n",
        "            except ValueError as e:\n",
        "                print(f\"Error calculating AUC-ROC for multi-class: {e}\")\n",
        "        elif y_pred_proba.shape[1] > 1:  # Binary case with probability scores\n",
        "            auc_roc = roc_auc_score(y_true, y_pred_proba[:, 1])\n",
        "        elif y_pred_proba.shape[1] == 1: # Binary case with single probability score\n",
        "            auc_roc = roc_auc_score(y_true, y_pred_proba[:, 0])\n",
        "\n",
        "    return accuracy, f1, auc_roc\n",
        "\n",
        "# Evaluate models on the test data\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nEvaluating {name}...\")\n",
        "    if name in ['VGG19', 'ResNet50', 'DenseNet', 'CNN']:\n",
        "        accuracy, f1, auc_roc = evaluate_model(model, X_test_dl, y_test_onehot, is_deep_learning=True)\n",
        "    elif name == 'SVM':\n",
        "        accuracy, f1, auc_roc = evaluate_model(model, X_test_gabor, y_test_int, is_deep_learning=False)\n",
        "    elif name == 'RandomForest':\n",
        "        accuracy, f1, auc_roc = evaluate_model(model, X_test_hog, y_test_int, is_deep_learning=False)\n",
        "\n",
        "    results[name] = {'Accuracy': accuracy, 'F1-Score': f1, 'AUC-ROC': auc_roc}\n",
        "    print(f\"{name}: Accuracy = {accuracy:.4f}, F1-Score = {f1:.4f}, AUC-ROC = {auc_roc:.4f}\")\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(\"\\nModel Performance on Test Data:\")\n",
        "print(results_df)\n",
        "\n",
        "# Select top 3 models based on a combined score\n",
        "results_df['Combined_Score'] = (results_df['Accuracy'] + results_df['F1-Score'] + results_df['AUC-ROC']) / 3\n",
        "top_3_models = results_df.nlargest(3, 'Combined_Score').index.tolist()\n",
        "print(\"\\nTop 3 Models on Test Data:\", top_3_models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlSzFtQJKK36",
        "outputId": "5933bc9a-d3b9-4bd0-c8b0-7df33fb89623"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 589ms/step\n"
          ]
        }
      ],
      "source": [
        "# Get predictions from top 3 models\n",
        "y_preds = []\n",
        "\n",
        "for model_name in top_3_models:\n",
        "    model = models[model_name]\n",
        "    \n",
        "    if model_name in ['VGG19', 'ResNet50', 'DenseNet', 'CNN']:\n",
        "        # For deep learning models, get softmax probabilities\n",
        "        y_pred_proba = model.predict(X_val_dl_full)\n",
        "    \n",
        "    else:\n",
        "        # For SVM and Random Forest, use extracted features\n",
        "        X_val_fe = X_val_gabor if model_name == 'SVM' else X_val_hog\n",
        "        y_pred_proba = model.predict_proba(X_val_fe)\n",
        "\n",
        "    y_preds.append(y_pred_proba)\n",
        "\n",
        "# Convert to NumPy array for further processing\n",
        "y_preds = np.array(y_preds)  # Shape: (3, num_samples, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "z9e0WypedOd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🎯 Weighted Averaging Hybrid Model Performance:\n",
            "✅ Accuracy: 0.9228\n",
            "✅ F1-Score: 0.9233\n",
            "✅ AUC-ROC: 0.9913\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "# Normalize weights based on Combined_Score\n",
        "weights = results_df.loc[top_3_models, 'Combined_Score'].values\n",
        "weights /= np.sum(weights)  # Ensure sum of weights is 1\n",
        "\n",
        "# Perform weighted averaging\n",
        "y_weighted_avg = np.tensordot(weights, y_preds, axes=(0, 0))  # Shape: (num_samples, num_classes)\n",
        "\n",
        "# Get final class predictions\n",
        "y_weighted_avg_classes = np.argmax(y_weighted_avg, axis=1)\n",
        "\n",
        "# Evaluate the weighted averaging hybrid model\n",
        "y_true = np.argmax(y_val_dl_full, axis=1)  # Convert one-hot to integer labels\n",
        "\n",
        "accuracy_weighted = accuracy_score(y_true, y_weighted_avg_classes)\n",
        "f1_weighted = f1_score(y_true, y_weighted_avg_classes, average='weighted')\n",
        "\n",
        "# Handle AUC-ROC correctly for multi-class classification\n",
        "auc_roc_weighted = roc_auc_score(y_true, y_weighted_avg, multi_class='ovr')\n",
        "\n",
        "# Display results\n",
        "print(\"\\n🎯 Weighted Averaging Hybrid Model Performance:\")\n",
        "print(f\"✅ Accuracy: {accuracy_weighted:.4f}\")\n",
        "print(f\"✅ F1-Score: {f1_weighted:.4f}\")\n",
        "print(f\"✅ AUC-ROC: {auc_roc_weighted:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqbsvn9ZdYcC",
        "outputId": "252ff143-cf82-45d2-ddf4-6af1af52ae11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🚀 Stacking Hybrid Model Performance:\n",
            "✅ Accuracy: 0.9637\n",
            "✅ F1-Score: 0.9636\n",
            "✅ AUC-ROC: 0.9975\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Admin\\Desktop\\CerebralFusion\\cerebralFusion_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Convert validation labels to class indices\n",
        "y_true = np.argmax(y_val_dl_full, axis=1)\n",
        "\n",
        "# Stack predictions from top 3 models\n",
        "X_stack = np.hstack(y_preds)  # Ensure (num_samples, num_classes * num_models) shape\n",
        "\n",
        "# Standardize the stacked predictions (helps with Logistic Regression stability)\n",
        "scaler = StandardScaler()\n",
        "X_stack_scaled = scaler.fit_transform(X_stack)\n",
        "\n",
        "# Train a meta-learner (Logistic Regression)\n",
        "meta_learner = LogisticRegression(max_iter=500, multi_class='multinomial', solver='lbfgs', random_state=42)\n",
        "meta_learner.fit(X_stack_scaled, y_true)\n",
        "\n",
        "# Evaluate stacking hybrid model\n",
        "y_stack_pred = meta_learner.predict(X_stack_scaled)\n",
        "y_stack_pred_proba = meta_learner.predict_proba(X_stack_scaled)\n",
        "\n",
        "# Calculate performance metrics\n",
        "accuracy_stack = accuracy_score(y_true, y_stack_pred)\n",
        "f1_stack = f1_score(y_true, y_stack_pred, average='weighted')\n",
        "auc_roc_stack = roc_auc_score(y_true, y_stack_pred_proba, multi_class='ovr')\n",
        "\n",
        "# Display results\n",
        "print(\"\\n🚀 Stacking Hybrid Model Performance:\")\n",
        "print(f\"✅ Accuracy: {accuracy_stack:.4f}\")\n",
        "print(f\"✅ F1-Score: {f1_stack:.4f}\")\n",
        "print(f\"✅ AUC-ROC: {auc_roc_stack:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZQgLuNqdgHS",
        "outputId": "56cda657-ac8c-42fa-fe93-c1fc898355c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of y_pred_classes: (3, 855)\n",
            "Shape of y_avg_proba: (855, 4)\n",
            "Shape of y_val_dl_full: (855, 4)\n",
            "\n",
            "🗳️ Majority Voting Hybrid Model Performance:\n",
            "✅ Accuracy: 0.9263\n",
            "✅ F1-Score: 0.9265\n",
            "✅ AUC-ROC: 0.9915\n"
          ]
        }
      ],
      "source": [
        "from scipy.stats import mode\n",
        "\n",
        "# Majority voting\n",
        "y_pred_classes = np.array([np.argmax(y_pred, axis=1) for y_pred in y_preds])  # Shape: (num_models, num_samples)\n",
        "\n",
        "# ✅ Fix for SciPy 1.11+ (mode now returns a named tuple)\n",
        "y_majority_vote = mode(y_pred_classes, axis=0, keepdims=True).mode.flatten()\n",
        "\n",
        "# Compute average probability predictions for AUC-ROC\n",
        "y_avg_proba = np.mean(y_preds, axis=0)  # Average probabilities across models\n",
        "\n",
        "# ✅ Verify correct shapes\n",
        "print(\"Shape of y_pred_classes:\", y_pred_classes.shape)  # (num_models, num_samples)\n",
        "print(\"Shape of y_avg_proba:\", y_avg_proba.shape)  # (num_samples, num_classes)\n",
        "print(\"Shape of y_val_dl_full:\", y_val_dl_full.shape)  # (num_samples, num_classes)\n",
        "\n",
        "# Convert validation labels to class indices\n",
        "y_true = np.argmax(y_val_dl_full, axis=1)\n",
        "\n",
        "# ✅ Evaluate majority voting hybrid model\n",
        "accuracy_majority = accuracy_score(y_true, y_majority_vote)\n",
        "f1_majority = f1_score(y_true, y_majority_vote, average='weighted')\n",
        "auc_roc_majority = roc_auc_score(y_true, y_avg_proba, multi_class='ovr')\n",
        "\n",
        "# ✅ Display results\n",
        "print(\"\\n🗳️ Majority Voting Hybrid Model Performance:\")\n",
        "print(f\"✅ Accuracy: {accuracy_majority:.4f}\")\n",
        "print(f\"✅ F1-Score: {f1_majority:.4f}\")\n",
        "print(f\"✅ AUC-ROC: {auc_roc_majority:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FO7edBjefkeH",
        "outputId": "bb628ea7-0df6-45c7-ec0d-901665c89b51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Hybrid Model Performance Comparison:\n",
            "                    Accuracy  F1-Score  AUC-ROC  Combined_Score\n",
            "Weighted Averaging    0.9228    0.9233   0.9913          0.9458\n",
            "Stacking              0.9637    0.9636   0.9975          0.9749\n",
            "Majority Voting       0.9263    0.9265   0.9915          0.9481\n",
            "\n",
            "🏆 Best Hybrid Model: Stacking\n"
          ]
        }
      ],
      "source": [
        "# 📊 Compare hybrid model results\n",
        "hybrid_results = {\n",
        "    'Weighted Averaging': {\n",
        "        'Accuracy': accuracy_weighted,\n",
        "        'F1-Score': f1_weighted,\n",
        "        'AUC-ROC': auc_roc_weighted\n",
        "    },\n",
        "    'Stacking': {\n",
        "        'Accuracy': accuracy_stack,\n",
        "        'F1-Score': f1_stack,\n",
        "        'AUC-ROC': auc_roc_stack\n",
        "    },\n",
        "    'Majority Voting': {\n",
        "        'Accuracy': accuracy_majority,\n",
        "        'F1-Score': f1_majority,\n",
        "        'AUC-ROC': auc_roc_majority\n",
        "    }\n",
        "}\n",
        "\n",
        "# ✅ Convert to DataFrame for easy visualization\n",
        "hybrid_results_df = pd.DataFrame(hybrid_results).T\n",
        "\n",
        "# ✅ Compute a Combined Score (mean of all metrics)\n",
        "hybrid_results_df['Combined_Score'] = hybrid_results_df.mean(axis=1)\n",
        "\n",
        "# ✅ Display results\n",
        "print(\"\\n📊 Hybrid Model Performance Comparison:\")\n",
        "print(hybrid_results_df.round(4))  # Round values for better readability\n",
        "\n",
        "# ✅ Select the best hybrid model based on Combined Score\n",
        "best_hybrid_model = hybrid_results_df['Combined_Score'].idxmax()\n",
        "print(\"\\n🏆 Best Hybrid Model:\", best_hybrid_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "i_wWx7XKGuCS",
        "outputId": "416dba08-4436-4184-c866-781368da3934"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Stacking hybrid model not found. Training...\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 621ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 628ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 636ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 652ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 629ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 648ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 630ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 639ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 665ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 646ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 636ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 636ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 646ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 618ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 645ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 646ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 653ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 655ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 648ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 651ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 643ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 626ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 651ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 638ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 628ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 667ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 680ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 637ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 666ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 647ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 631ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 661ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 628ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 637ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 652ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 634ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 660ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 631ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 639ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 667ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 626ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 655ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 643ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 651ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 625ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 640ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 669ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 623ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 658ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 627ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 641ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 640ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 622ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 637ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 670ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 645ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 659ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 665ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 641ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 651ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 650ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 631ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 635ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 644ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 646ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 642ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 647ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 630ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 638ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 648ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 635ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 647ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 627ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 661ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 624ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 629ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 659ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 625ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 643ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 622ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 644ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 641ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 624ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 653ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 623ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 647ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 639ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 625ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 638ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 654ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 620ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 662ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 623ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 632ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 639ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 624ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 657ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 639ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 657ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 630ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 664ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 658ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 660ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 639ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 649ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 649ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 661ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 690ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 642ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 661ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 657ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 645ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 671ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 621ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 622ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 665ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 618ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 633ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 622ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 617ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 671ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 627ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 634ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 662ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 631ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 636ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 656ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 636ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 631ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 659ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 642ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 636ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 633ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 642ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 653ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 638ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 631ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 639ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 648ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 626ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 620ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 615ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 634ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 639ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 616ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 633ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 618ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 622ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 652ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 614ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 633ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 635ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 607ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 626ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 615ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 625ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 676ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 608ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 631ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 654ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 615ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 617ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 634ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 621ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 640ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 632ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 626ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 618ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 626ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 619ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 639ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 627ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 617ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 619ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 629ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 635ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 628ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 633ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345ms/step\n",
            "Feature extractor output shape: (None, 4)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 786ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 799ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 793ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 775ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 782ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 781ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 793ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 784ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 784ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 792ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 799ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 800ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 792ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 786ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 773ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 774ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 798ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 790ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 801ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 774ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 769ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 773ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 786ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 793ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 777ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 778ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 784ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 779ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 788ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 776ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 778ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 773ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 792ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 779ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 867ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 807ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 786ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 782ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 765ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 783ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 784ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 785ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 769ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 791ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 787ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 763ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 775ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 789ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 813ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 790ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 774ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 788ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 781ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 775ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 792ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 796ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 789ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 816ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 795ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 784ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 787ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 811ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 781ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 786ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 819ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 813ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 822ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 801ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 807ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 800ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 787ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 792ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 784ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 786ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 791ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 818ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 827ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 801ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 796ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 802ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 826ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 844ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 798ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 817ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 798ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 787ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 813ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 802ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 830ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 828ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 799ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 816ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 821ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 800ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 799ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 802ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 815ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 826ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 785ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 880ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 830ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 874ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 864ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 871ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 822ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 780ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 796ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 795ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 797ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 790ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 837ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 821ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 805ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 803ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 819ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 887ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 794ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 821ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 808ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 817ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 831ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 847ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 878ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 866ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 870ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 882ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 876ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 897ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 875ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 830ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 788ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 805ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 790ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 792ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 801ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 780ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 780ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 785ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 793ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 796ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 787ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 784ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 788ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 821ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 811ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 792ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 779ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 794ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 798ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 777ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 781ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 802ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 808ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 816ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 787ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 791ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 795ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 837ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 830ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 852ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 815ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 841ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 821ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 789ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 802ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 816ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 792ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 805ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 783ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 783ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 785ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 800ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 785ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 784ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 793ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 787ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 775ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 786ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 444ms/step\n",
            "✅ Stacking model saved at: C:\\Users\\Admin\\Desktop\\CerebralFusion\\Stacking_hybrid_model.pkl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Admin\\Desktop\\CerebralFusion\\cerebralFusion_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🚀 Hybrid model training & evaluation completed!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import gc\n",
        "import joblib\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "from scipy.stats import mode\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
        "\n",
        "# 📁 Define the local directory for saving models\n",
        "model_save_dir = r'C:\\Users\\Admin\\Desktop\\CerebralFusion'\n",
        "os.makedirs(model_save_dir, exist_ok=True)\n",
        "\n",
        "# 📌 Define best hybrid model\n",
        "best_hybrid_model = 'Stacking'  # Change to 'Weighted Averaging' or 'Majority Voting' as needed\n",
        "\n",
        "# 📌 Hybrid model path\n",
        "hybrid_model_path = os.path.join(model_save_dir, f'{best_hybrid_model}_hybrid_model.pkl')\n",
        "\n",
        "# ✅ Define training parameters\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 20\n",
        "\n",
        "# ✅ Function to collect full dataset\n",
        "def collect_full_data(generator):\n",
        "    X_full, y_full = [], []\n",
        "    generator.reset()\n",
        "    for i in range(len(generator)):\n",
        "        X_batch, y_batch = generator[i]\n",
        "        X_full.append(X_batch)\n",
        "        y_full.append(y_batch)\n",
        "    return np.vstack(X_full), np.vstack(y_full)\n",
        "\n",
        "def get_predictions_in_batches(model, generator, is_deep_learning=False):\n",
        "    y_pred_proba = []\n",
        "    generator.reset()\n",
        "    \n",
        "    # Initialize feature extractor if needed\n",
        "    if not is_deep_learning and 'VGG19' in models:\n",
        "        feature_extractor = models['VGG19']\n",
        "        print(f\"Feature extractor output shape: {feature_extractor.output_shape}\")\n",
        "    \n",
        "    for i in range(len(generator)):\n",
        "        X_batch, _ = generator[i]\n",
        "\n",
        "        if not is_deep_learning:\n",
        "            X_batch = preprocess_input(X_batch.copy())  \n",
        "            X_batch = feature_extractor.predict(X_batch)\n",
        "            X_batch = X_batch.reshape(X_batch.shape[0], -1)\n",
        "\n",
        "            # Ensure dimension match\n",
        "            if X_batch.shape[1] > model.n_features_in_:\n",
        "                X_batch = X_batch[:, :model.n_features_in_]  # truncate\n",
        "            elif X_batch.shape[1] < model.n_features_in_:\n",
        "                padding = np.zeros((X_batch.shape[0], model.n_features_in_ - X_batch.shape[1]))\n",
        "                X_batch = np.hstack([X_batch, padding])\n",
        "\n",
        "        # Get predictions\n",
        "        if is_deep_learning:\n",
        "            y_pred_proba_batch = model.predict(X_batch)\n",
        "        else:\n",
        "            y_pred_proba_batch = model.predict_proba(X_batch) if hasattr(model, 'predict_proba') else model.predict(X_batch)\n",
        "            \n",
        "        y_pred_proba.append(y_pred_proba_batch)\n",
        "    \n",
        "    return np.vstack(y_pred_proba)\n",
        "\n",
        "# 🔍 Check if hybrid model exists\n",
        "if os.path.exists(hybrid_model_path):\n",
        "    print(f\"✅ {best_hybrid_model} hybrid model already exists. Loading...\")\n",
        "    if best_hybrid_model == 'Stacking':\n",
        "        meta_learner = joblib.load(hybrid_model_path)\n",
        "else:\n",
        "    print(f\"🚀 {best_hybrid_model} hybrid model not found. Training...\")\n",
        "\n",
        "    if best_hybrid_model == 'Weighted Averaging':\n",
        "        y_preds_test = [get_predictions_in_batches(models[model], test_generator, model in ['VGG19', 'ResNet50', 'DenseNet', 'CNN'])\n",
        "                        for model in top_3_models]\n",
        "\n",
        "        # 🔢 Weighted Averaging\n",
        "        weights = results_df.loc[top_3_models, 'Combined_Score'].values\n",
        "        weights /= weights.sum()\n",
        "        y_weighted_avg_test = np.average(y_preds_test, axis=0, weights=weights)\n",
        "        y_weighted_avg_classes_test = np.argmax(y_weighted_avg_test, axis=1)\n",
        "\n",
        "        # 📊 Evaluate\n",
        "        accuracy_test = accuracy_score(np.argmax(y_test_dl_full, axis=1), y_weighted_avg_classes_test)\n",
        "        f1_test = f1_score(np.argmax(y_test_dl_full, axis=1), y_weighted_avg_classes_test, average='weighted')\n",
        "        auc_roc_test = roc_auc_score(np.argmax(y_test_dl_full, axis=1), y_weighted_avg_test, multi_class='ovr')\n",
        "\n",
        "        print(\"\\n📊 Weighted Averaging Hybrid Model Performance:\")\n",
        "        print(f\"Accuracy: {accuracy_test}, F1-Score: {f1_test}, AUC-ROC: {auc_roc_test}\")\n",
        "\n",
        "    elif best_hybrid_model == 'Stacking':\n",
        "        # 🔄 Collect full dataset\n",
        "        X_train_dl_full, y_train_dl_full = collect_full_data(train_generator_aug)\n",
        "\n",
        "        # 🎯 Get training predictions from top 3 models\n",
        "        y_preds_train = [get_predictions_in_batches(models[model], train_generator_aug, model in ['VGG19', 'ResNet50', 'DenseNet', 'CNN'])\n",
        "                         for model in top_3_models]\n",
        "\n",
        "        # 🔢 Stacked Input\n",
        "        X_stack_train = np.hstack(y_preds_train)\n",
        "\n",
        "        # 🏋️‍♂️ Train Meta-Learner\n",
        "        meta_learner.fit(X_stack_train, np.argmax(y_train_dl_full, axis=1))\n",
        "\n",
        "        # 💾 Save model\n",
        "        joblib.dump(meta_learner, hybrid_model_path)\n",
        "        print(f\"✅ Stacking model saved at: {hybrid_model_path}\")\n",
        "\n",
        "        # 🗑️ Memory Cleanup\n",
        "        del X_train_dl_full, y_train_dl_full, y_preds_train, X_stack_train\n",
        "        gc.collect()\n",
        "\n",
        "    elif best_hybrid_model == 'Majority Voting':\n",
        "        y_preds_test = [get_predictions_in_batches(models[model], test_generator, model in ['VGG19', 'ResNet50', 'DenseNet', 'CNN'])\n",
        "                        for model in top_3_models]\n",
        "\n",
        "        # 🗳️ Majority Voting\n",
        "        y_pred_classes_test = np.array([np.argmax(y_pred, axis=1) for y_pred in y_preds_test])\n",
        "        y_majority_vote_test, _ = mode(y_pred_classes_test, axis=0)\n",
        "        y_majority_vote_test = y_majority_vote_test.flatten()\n",
        "\n",
        "        # 📊 Evaluate\n",
        "        accuracy_test = accuracy_score(np.argmax(y_test_dl_full, axis=1), y_majority_vote_test)\n",
        "        f1_test = f1_score(np.argmax(y_test_dl_full, axis=1), y_majority_vote_test, average='weighted')\n",
        "        auc_roc_test = roc_auc_score(np.argmax(y_test_dl_full, axis=1), np.mean(y_preds_test, axis=0), multi_class='ovr')\n",
        "\n",
        "        print(\"\\n📊 Majority Voting Hybrid Model Performance:\")\n",
        "        print(f\"Accuracy: {accuracy_test}, F1-Score: {f1_test}, AUC-ROC: {auc_roc_test}\")\n",
        "\n",
        "# ✅ Hybrid Model Completed\n",
        "print(\"\\n🚀 Hybrid model training & evaluation completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMMFPjzDjQ26",
        "outputId": "23ea6424-002f-4149-8b12-b4e1bae5d433"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import joblib\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "\n",
        "# 📁 Define the local directory for saving models\n",
        "model_save_dir = r'C:\\Users\\Admin\\Desktop\\CerebralFusion'\n",
        "os.makedirs(model_save_dir, exist_ok=True)\n",
        "\n",
        "def save_models(best_hybrid_model, models, top_3_models, meta_learner=None):\n",
        "    \"\"\"\n",
        "    Save models based on the hybrid approach used\n",
        "    \n",
        "    Parameters:\n",
        "    - best_hybrid_model: str ('Weighted_Averaging', 'Stacking', or 'Majority_Voting')\n",
        "    - models: dict of trained models\n",
        "    - top_3_models: list of model names to save\n",
        "    - meta_learner: only needed for Stacking approach\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"\\nStarting model save process in: {model_save_dir}\")\n",
        "        \n",
        "        if best_hybrid_model == 'Weighted_Averaging':\n",
        "            print(\"💾 Saving Weighted Averaging models...\")\n",
        "            for model_name in top_3_models:\n",
        "                save_single_model(models[model_name], model_name)\n",
        "                \n",
        "        elif best_hybrid_model == 'Stacking':\n",
        "            print(\"💾 Saving Stacking models and meta-learner...\")\n",
        "            \n",
        "            # Debug: Check meta-learner exists\n",
        "            if meta_learner is None:\n",
        "                raise ValueError(\"Meta-learner is None! Please provide a trained meta-learner for Stacking\")\n",
        "            print(f\"Meta-learner type: {type(meta_learner)}\")\n",
        "            \n",
        "            # First save base models\n",
        "            for model_name in top_3_models:\n",
        "                save_single_model(models[model_name], model_name)\n",
        "                \n",
        "            # Then save meta-learner\n",
        "            meta_learner_path = os.path.join(model_save_dir, 'stacking_meta_learner.pkl')\n",
        "            print(f\"Attempting to save meta-learner to: {meta_learner_path}\")\n",
        "            \n",
        "            joblib.dump(meta_learner, meta_learner_path)\n",
        "            print(f\"✅ Successfully saved meta-learner to {meta_learner_path}\")\n",
        "            \n",
        "        elif best_hybrid_model == 'Majority_Voting':\n",
        "            print(\"💾 Saving Majority Voting models...\")\n",
        "            for model_name in top_3_models:\n",
        "                save_single_model(models[model_name], model_name)\n",
        "                \n",
        "        else:\n",
        "            raise ValueError(f\"Unknown hybrid model type: {best_hybrid_model}\")\n",
        "            \n",
        "        print(f\"\\n✅ All models saved successfully in: {model_save_dir}\")\n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ Error saving models: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "def save_single_model(model, model_name):\n",
        "    \"\"\"Helper function to save individual models\"\"\"\n",
        "    try:\n",
        "        # Determine file extension\n",
        "        is_dl_model = model_name in ['VGG19', 'ResNet50', 'DenseNet', 'CNN']\n",
        "        ext = '.h5' if is_dl_model else '.pkl'\n",
        "        model_path = os.path.join(model_save_dir, f'{model_name}_model{ext}')\n",
        "        \n",
        "        print(f\"Attempting to save {model_name} to: {model_path}\")\n",
        "        \n",
        "        # Save model\n",
        "        if is_dl_model:\n",
        "            model.save(model_path)\n",
        "            print(f\"✅ Saved {model_name} (Keras) to {model_path}\")\n",
        "        else:\n",
        "            joblib.dump(model, model_path)\n",
        "            print(f\"✅ Saved {model_name} (Scikit-learn) to {model_path}\")\n",
        "            \n",
        "        return model_path\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to save {model_name}: {str(e)}\")\n",
        "        raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "RZkcuQpiZjVF",
        "outputId": "71293536-fcd8-4fd9-90eb-59c55a6a2f5a"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[53], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mode\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mskimage\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hog  \u001b[38;5;66;03m# Import HOG feature extractor\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_hog_features\u001b[39m(image):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Ensure the image is large enough for HOG feature extraction\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import joblib\n",
        "from tensorflow.keras.models import load_model\n",
        "from scipy.stats import mode\n",
        "from google.colab import files\n",
        "from skimage.feature import hog  # Import HOG feature extractor\n",
        "\n",
        "def extract_hog_features(image):\n",
        "    # Ensure the image is large enough for HOG feature extraction\n",
        "    min_size = 16  # Minimum size required by HOG\n",
        "    if image.shape[0] < min_size or image.shape[1] < min_size:\n",
        "        # Resize the image to meet the minimum size requirement\n",
        "        image = cv2.resize(image, (min_size, min_size))\n",
        "\n",
        "    # Extract HOG features\n",
        "    features = hog(\n",
        "        image,\n",
        "        orientations=9,  # Number of orientation bins\n",
        "        pixels_per_cell=(8, 8),  # Size of a cell\n",
        "        cells_per_block=(2, 2),  # Number of cells in each block\n",
        "        block_norm='L2-Hys',  # Normalization method\n",
        "        visualize=False,  # Do not return the HOG image\n",
        "        feature_vector=True  # Return features as a 1D array\n",
        "    )\n",
        "    return features.reshape(1, -1)  # Reshape to match input format for classifiers\n",
        "\n",
        "def predict_with_hybrid_model(image_path):\n",
        "    # Load the image and preprocess it\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Load as grayscale\n",
        "    img = cv2.resize(img, (224, 224))  # Resize to 224x224\n",
        "    img = img / 255.0  # Normalize\n",
        "    img = np.expand_dims(img, axis=-1)  # Add channel dimension (1 channel)\n",
        "    img = np.repeat(img, 3, axis=-1)  # Convert to 3 channels (RGB)\n",
        "    img = np.expand_dims(img, axis=0)  # Add batch dimension (1, 224, 224, 3)\n",
        "\n",
        "    # Load the top 3 models\n",
        "    top_models = {}\n",
        "    for model_name in top_3_models:\n",
        "        if model_name in ['VGG19', 'ResNet50', 'DenseNet', 'CNN']:\n",
        "            top_models[model_name] = load_model(f'{model_name}_model.h5')\n",
        "        else:\n",
        "            top_models[model_name] = joblib.load(f'{model_name}_model.pkl')\n",
        "\n",
        "    # Get predictions from the top 3 models\n",
        "    y_preds = []\n",
        "    for model_name, model in top_models.items():\n",
        "        if model_name in ['VGG19', 'ResNet50', 'DenseNet', 'CNN']:\n",
        "            y_pred_proba = model.predict(img)  # Predict using deep learning models\n",
        "        else:\n",
        "            if model_name == 'SVM':\n",
        "                features = extract_gabor_features(img[0, :, :, 0])  # Extract features from grayscale\n",
        "            else:\n",
        "                features = extract_hog_features(img[0, :, :, 0])  # Extract features from grayscale\n",
        "            y_pred_proba = model.predict_proba(features)\n",
        "        y_preds.append(y_pred_proba)\n",
        "\n",
        "    # Combine predictions based on the best hybrid model\n",
        "    if best_hybrid_model == 'Weighted Averaging':\n",
        "        weights = results_df.loc[top_3_models, 'Combined_Score'].values\n",
        "        weights /= weights.sum()  # Normalize weights\n",
        "        y_weighted_avg = np.average(y_preds, axis=0, weights=weights)\n",
        "        final_pred = np.argmax(y_weighted_avg, axis=1)\n",
        "    elif best_hybrid_model == 'Stacking':\n",
        "        X_stack = np.hstack(y_preds)\n",
        "        meta_learner = joblib.load('stacking_meta_learner.pkl')\n",
        "        final_pred = meta_learner.predict(X_stack)\n",
        "    elif best_hybrid_model == 'Majority Voting':\n",
        "        y_pred_classes = np.array([np.argmax(y_pred, axis=1) for y_pred in y_preds])\n",
        "        final_pred, _ = mode(y_pred_classes, axis=0)\n",
        "        final_pred = final_pred.flatten()\n",
        "\n",
        "    # Map prediction to class label\n",
        "    class_labels = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "    return class_labels[final_pred[0]]\n",
        "\n",
        "# Upload an image using Google Colab's file uploader\n",
        "uploaded = files.upload()\n",
        "image_path = next(iter(uploaded))\n",
        "\n",
        "# Make a prediction\n",
        "if image_path:\n",
        "    prediction = predict_with_hybrid_model(image_path)\n",
        "    print(\"Predicted Class:\", prediction)\n",
        "else:\n",
        "    print(\"No image selected.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading models...\n",
            "Warning: VGG16 has unexpected input shape (None, 5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded DenseNet successfully\n",
            "Loaded RandomForest successfully\n",
            "\n",
            "Loading test data...\n",
            "Loading glioma images...\n",
            "Loading meningioma images...\n",
            "Loading notumor images...\n",
            "Loading pituitary images...\n",
            "\n",
            "Loaded 1311 test samples\n",
            "\n",
            "Starting evaluations...\n",
            "\n",
            "Evaluating DenseNet...\n",
            "Predicting with DenseNet...\n",
            "DenseNet Accuracy: 0.9481\n",
            "\n",
            "Evaluating RandomForest...\n",
            "Predicting with RandomForest...\n",
            "RandomForest Accuracy: 0.8940\n",
            "\n",
            "Evaluating Stacking Hybrid Model...\n",
            "Stacking Hybrid Model Accuracy: 0.9603\n",
            "\n",
            "\n",
            "=== Evaluation Summary ===\n",
            "\n",
            "Individual Models:\n",
            "DenseNet: 0.9481\n",
            "RandomForest: 0.8940\n",
            "\n",
            "Stacking Hybrid Model: 0.9603\n",
            "\n",
            "Results saved to C:\\Users\\Admin\\Desktop\\CerebralFusion\\model_evaluation_results.txt\n"
          ]
        }
      ],
      "source": [
        "# import cv2\n",
        "# import numpy as np\n",
        "# import joblib\n",
        "# from tensorflow.keras.models import load_model\n",
        "# from sklearn.metrics import accuracy_score, classification_report\n",
        "# from skimage.feature import hog\n",
        "# from sklearn.ensemble import StackingClassifier\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# import os\n",
        "\n",
        "# # Configuration\n",
        "# model_mapping = {\n",
        "#     'VGG16': 'vgg16_model.h5',\n",
        "#     'DenseNet': 'densenet121_model.h5',\n",
        "#     'RandomForest': 'random_forest_model.pkl'\n",
        "# }\n",
        "\n",
        "# class_labels = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "\n",
        "# # Path configuration\n",
        "# model_dir = r'C:\\Users\\Admin\\Desktop\\CerebralFusion'\n",
        "# test_dir = r'C:\\Users\\Admin\\Desktop\\CerebralFusion\\Brain_Tumor_Dataset\\Testing'\n",
        "\n",
        "# # Custom metric function to handle loading\n",
        "# def custom_mse(y_true, y_pred):\n",
        "#     from keras import backend as K\n",
        "#     return K.mean(K.square(y_pred - y_true), axis=-1)\n",
        "\n",
        "# # Load models with proper error handling\n",
        "# print(\"Loading models...\")\n",
        "# models = {}\n",
        "# loaded_models = {}\n",
        "\n",
        "# for model_name, model_file in model_mapping.items():\n",
        "#     model_path = os.path.join(model_dir, model_file)\n",
        "#     try:\n",
        "#         if model_path.endswith('.h5'):\n",
        "#             model = load_model(model_path, custom_objects={'mse': custom_mse})\n",
        "#             # Verify model input shape\n",
        "#             if model_name == 'VGG16' and model.input_shape[1:] != (224, 224, 3):\n",
        "#                 print(f\"Warning: {model_name} has unexpected input shape {model.input_shape}\")\n",
        "#                 continue\n",
        "#             loaded_models[model_name] = model\n",
        "#         else:\n",
        "#             loaded_models[model_name] = joblib.load(model_path)\n",
        "#         print(f\"Loaded {model_name} successfully\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error loading {model_name}: {str(e)}\")\n",
        "\n",
        "# # Load test data with proper preprocessing\n",
        "# def load_test_data(test_dir):\n",
        "#     X_test = []\n",
        "#     y_test = []\n",
        "#     print(\"\\nLoading test data...\")\n",
        "    \n",
        "#     for class_idx, class_name in enumerate(class_labels):\n",
        "#         class_dir = os.path.join(test_dir, class_name)\n",
        "#         if not os.path.exists(class_dir):\n",
        "#             continue\n",
        "            \n",
        "#         print(f\"Loading {class_name} images...\")\n",
        "#         for img_file in os.listdir(class_dir):\n",
        "#             img_path = os.path.join(class_dir, img_file)\n",
        "#             try:\n",
        "#                 img = cv2.imread(img_path)\n",
        "#                 if img is None:\n",
        "#                     continue\n",
        "                    \n",
        "#                 img = cv2.resize(img, (224, 224))\n",
        "#                 img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "#                 img = img.astype(np.float32) / 255.0  # Normalize\n",
        "                \n",
        "#                 X_test.append(img)\n",
        "#                 y_test.append(class_idx)\n",
        "#             except Exception as e:\n",
        "#                 print(f\"Error loading {img_path}: {str(e)}\")\n",
        "    \n",
        "#     return np.array(X_test), np.array(y_test)\n",
        "\n",
        "# X_test, y_test = load_test_data(test_dir)\n",
        "# print(f\"\\nLoaded {len(X_test)} test samples\")\n",
        "\n",
        "# # Feature extraction with proper checks\n",
        "# def extract_hog_features(image):\n",
        "#     if len(image.shape) > 2:\n",
        "#         image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "#     if image.dtype != np.uint8:\n",
        "#         image = (image * 255).astype(np.uint8)\n",
        "#     features = hog(\n",
        "#         image,\n",
        "#         orientations=9,\n",
        "#         pixels_per_cell=(8, 8),\n",
        "#         cells_per_block=(2, 2),\n",
        "#         block_norm='L2-Hys',\n",
        "#         visualize=False,\n",
        "#         feature_vector=True\n",
        "#     )\n",
        "#     return features\n",
        "\n",
        "# # Evaluation functions with proper model handling\n",
        "# def evaluate_individual_models(models, X_test, y_test):\n",
        "#     results = {}\n",
        "    \n",
        "#     for model_name, model in models.items():\n",
        "#         print(f\"\\nEvaluating {model_name}...\")\n",
        "#         y_pred = []\n",
        "        \n",
        "#         print(f\"Predicting with {model_name}...\")\n",
        "#         for img in X_test:\n",
        "#             try:\n",
        "#                 if model_name in ['VGG16', 'DenseNet']:\n",
        "#                     # Ensure proper input shape\n",
        "#                     if model.input_shape[1:] != img.shape:\n",
        "#                         img_reshaped = cv2.resize(img, (model.input_shape[2], model.input_shape[1]))\n",
        "#                         img_reshaped = np.expand_dims(img_reshaped, axis=0)\n",
        "#                     else:\n",
        "#                         img_reshaped = np.expand_dims(img, axis=0)\n",
        "#                     pred = model.predict(img_reshaped, verbose=0)\n",
        "#                     y_pred.append(np.argmax(pred))\n",
        "#                 else:  # RandomForest\n",
        "#                     features = extract_hog_features(img)\n",
        "#                     pred = model.predict([features])\n",
        "#                     y_pred.append(pred[0])\n",
        "#             except Exception as e:\n",
        "#                 print(f\"Prediction error with {model_name}: {str(e)}\")\n",
        "#                 y_pred.append(0)  # Default prediction\n",
        "        \n",
        "#         acc = accuracy_score(y_test, y_pred)\n",
        "#         results[model_name] = {\n",
        "#             'accuracy': acc,\n",
        "#             'report': classification_report(y_test, y_pred, target_names=class_labels, zero_division=0)\n",
        "#         }\n",
        "#         print(f\"{model_name} Accuracy: {acc:.4f}\")\n",
        "    \n",
        "#     return results\n",
        "\n",
        "# def evaluate_stacking_hybrid_model(models, X_test, y_test):\n",
        "#     print(\"\\nEvaluating Stacking Hybrid Model...\")\n",
        "#     meta_features = []\n",
        "    \n",
        "#     for img in X_test:\n",
        "#         img_meta_features = []\n",
        "#         for model_name, model in models.items():\n",
        "#             try:\n",
        "#                 if model_name in ['VGG16', 'DenseNet']:\n",
        "#                     if model.input_shape[1:] != img.shape:\n",
        "#                         img_reshaped = cv2.resize(img, (model.input_shape[2], model.input_shape[1]))\n",
        "#                         img_reshaped = np.expand_dims(img_reshaped, axis=0)\n",
        "#                     else:\n",
        "#                         img_reshaped = np.expand_dims(img, axis=0)\n",
        "#                     pred = model.predict(img_reshaped, verbose=0)\n",
        "#                     img_meta_features.extend(pred[0])\n",
        "#                 else:  # RandomForest\n",
        "#                     features = extract_hog_features(img)\n",
        "#                     pred = model.predict_proba([features])\n",
        "#                     img_meta_features.extend(pred[0])\n",
        "#             except Exception as e:\n",
        "#                 print(f\"Error creating meta-features: {str(e)}\")\n",
        "#                 img_meta_features.extend([1.0/len(class_labels)]*len(class_labels))\n",
        "        \n",
        "#         meta_features.append(img_meta_features)\n",
        "    \n",
        "#     meta_features = np.array(meta_features)\n",
        "    \n",
        "#     # Train stacking classifier\n",
        "#     stacking_model = LogisticRegression(max_iter=1000)\n",
        "#     stacking_model.fit(meta_features, y_test)\n",
        "    \n",
        "#     # Evaluate\n",
        "#     y_pred = stacking_model.predict(meta_features)\n",
        "#     acc = accuracy_score(y_test, y_pred)\n",
        "    \n",
        "#     results = {\n",
        "#         'accuracy': acc,\n",
        "#         'report': classification_report(y_test, y_pred, target_names=class_labels, zero_division=0)\n",
        "#     }\n",
        "#     print(f\"Stacking Hybrid Model Accuracy: {acc:.4f}\")\n",
        "#     return results\n",
        "\n",
        "# # Main execution\n",
        "# if __name__ == \"__main__\":\n",
        "#     # Run evaluations\n",
        "#     print(\"\\nStarting evaluations...\")\n",
        "#     individual_results = evaluate_individual_models(loaded_models, X_test, y_test)\n",
        "\n",
        "#     # Only run stacking if we have at least 2 models\n",
        "#     if len(loaded_models) >= 2:\n",
        "#         stacking_results = evaluate_stacking_hybrid_model(loaded_models, X_test, y_test)\n",
        "#     else:\n",
        "#         print(\"\\nNot enough models loaded for stacking (need at least 2)\")\n",
        "#         stacking_results = None\n",
        "\n",
        "#     # Print summary\n",
        "#     print(\"\\n\\n=== Evaluation Summary ===\")\n",
        "#     print(\"\\nIndividual Models:\")\n",
        "#     for model_name in loaded_models.keys():\n",
        "#         print(f\"{model_name}: {individual_results[model_name]['accuracy']:.4f}\")\n",
        "\n",
        "#     if stacking_results:\n",
        "#         print(f\"\\nStacking Hybrid Model: {stacking_results['accuracy']:.4f}\")\n",
        "\n",
        "#     # Save results\n",
        "#     output_file = os.path.join(model_dir, 'model_evaluation_results.txt')\n",
        "#     with open(output_file, 'w') as f:\n",
        "#         f.write(\"=== Individual Model Results ===\\n\")\n",
        "#         for model_name in loaded_models.keys():\n",
        "#             f.write(f\"\\n{model_name} - Accuracy: {individual_results[model_name]['accuracy']:.4f}\\n\")\n",
        "#             f.write(individual_results[model_name]['report'])\n",
        "        \n",
        "#         if stacking_results:\n",
        "#             f.write(\"\\n\\n=== Stacking Hybrid Model Results ===\\n\")\n",
        "#             f.write(f\"\\nAccuracy: {stacking_results['accuracy']:.4f}\\n\")\n",
        "#             f.write(stacking_results['report'])\n",
        "\n",
        "#     print(f\"\\nResults saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading models...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded VGG19 successfully\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded DenseNet successfully\n",
            "Loaded CustomCNN successfully\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded ResNet50 successfully\n",
            "Loaded RandomForest successfully\n",
            "Loaded SVM successfully\n",
            "\n",
            "Loading test data...\n",
            "Loading glioma images...\n",
            "Loading meningioma images...\n",
            "Loading notumor images...\n",
            "Loading pituitary images...\n",
            "\n",
            "Loaded 1311 test samples\n",
            "\n",
            "Starting evaluations...\n",
            "\n",
            "Evaluating VGG19...\n",
            "Predicting with VGG19...\n",
            "VGG19 Accuracy: 0.8848, F1-score: 0.8829\n",
            "\n",
            "Evaluating DenseNet...\n",
            "Predicting with DenseNet...\n",
            "DenseNet Accuracy: 0.9481, F1-score: 0.9478\n",
            "\n",
            "Evaluating CustomCNN...\n",
            "Predicting with CustomCNN...\n",
            "CustomCNN Accuracy: 0.9603, F1-score: 0.9600\n",
            "\n",
            "Evaluating ResNet50...\n",
            "Predicting with ResNet50...\n",
            "ResNet50 Accuracy: 0.7689, F1-score: 0.7485\n",
            "\n",
            "Evaluating RandomForest...\n",
            "Predicting with RandomForest...\n",
            "RandomForest Accuracy: 0.8940, F1-score: 0.8924\n",
            "\n",
            "Evaluating SVM...\n",
            "Predicting with SVM...\n",
            "SVM Accuracy: 0.8268, F1-score: 0.8220\n",
            "\n",
            "Top 3 Models considered for stacking: ['CustomCNN', 'DenseNet', 'RandomForest']\n",
            "\n",
            "Evaluating Stacking Hybrid Model (Top Models Only)...\n",
            "Stacking Hybrid Model (Top) Accuracy: 0.9725, F1-score: 0.9724\n",
            "\n",
            "Stacking Hybrid Model (Top): Accuracy: 0.9725\n",
            "Training history file not found for VGG19 at C:\\Users\\Admin\\Desktop\\CerebralFusion\\training_history\\VGG19_history.pkl\n",
            "Training history file not found for DenseNet at C:\\Users\\Admin\\Desktop\\CerebralFusion\\training_history\\DenseNet_history.pkl\n",
            "Training history file not found for CustomCNN at C:\\Users\\Admin\\Desktop\\CerebralFusion\\training_history\\CustomCNN_history.pkl\n",
            "Training history file not found for ResNet50 at C:\\Users\\Admin\\Desktop\\CerebralFusion\\training_history\\ResNet50_history.pkl\n",
            "\n",
            "\n",
            "=== Evaluation Summary ===\n",
            "VGG19: Accuracy: 0.8848, F1-Score: 0.8829\n",
            "DenseNet: Accuracy: 0.9481, F1-Score: 0.9478\n",
            "CustomCNN: Accuracy: 0.9603, F1-Score: 0.9600\n",
            "ResNet50: Accuracy: 0.7689, F1-Score: 0.7485\n",
            "RandomForest: Accuracy: 0.8940, F1-Score: 0.8924\n",
            "SVM: Accuracy: 0.8268, F1-Score: 0.8220\n",
            "\n",
            "Stacking Hybrid Model (Top): Accuracy: 0.9725, F1-Score: 0.9724\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import joblib\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from skimage.feature import hog\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import f1_score\n",
        "import pandas as pd\n",
        "\n",
        "# Configuration\n",
        "model_mapping = {\n",
        "    'VGG19': 'vgg19_model.h5',\n",
        "    'DenseNet': 'densenet121_model.h5',\n",
        "    'CustomCNN': 'custom_cnn_model.h5',\n",
        "    'ResNet50': 'resnet50_model.h5',\n",
        "    'RandomForest': 'random_forest_model.pkl',\n",
        "    'SVM': 'svm_model.pkl'\n",
        "}\n",
        "\n",
        "class_labels = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "\n",
        "# Path configuration\n",
        "model_dir = r'C:\\Users\\Admin\\Desktop\\CerebralFusion'\n",
        "test_dir = r'C:\\Users\\Admin\\Desktop\\CerebralFusion\\Brain_Tumor_Dataset\\Testing'\n",
        "output_dir = r'C:\\Users\\Admin\\Desktop\\CerebralFusion\\model_comparison_plots'\n",
        "training_history_dir = r'C:\\Users\\Admin\\Desktop\\CerebralFusion\\training_history' # Assuming you have these\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "os.makedirs(training_history_dir, exist_ok=True)\n",
        "\n",
        "# Custom metric function to handle loading\n",
        "def custom_mse(y_true, y_pred):\n",
        "    from keras import backend as K\n",
        "    return K.mean(K.square(y_pred - y_true), axis=-1)\n",
        "\n",
        "# Load models\n",
        "print(\"Loading models...\")\n",
        "models = {}\n",
        "loaded_models = {}\n",
        "\n",
        "for model_name, model_file in model_mapping.items():\n",
        "    model_path = os.path.join(model_dir, model_file)\n",
        "    try:\n",
        "        if model_path.endswith('.h5'):\n",
        "            model = load_model(model_path, custom_objects={'mse': custom_mse})\n",
        "            loaded_models[model_name] = model\n",
        "        else:\n",
        "            loaded_models[model_name] = joblib.load(model_path)\n",
        "        print(f\"Loaded {model_name} successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {model_name}: {str(e)}\")\n",
        "\n",
        "# Load test data\n",
        "def load_test_data(test_dir):\n",
        "    X_test = []\n",
        "    y_test = []\n",
        "    print(\"\\nLoading test data...\")\n",
        "\n",
        "    for class_idx, class_name in enumerate(class_labels):\n",
        "        class_dir = os.path.join(test_dir, class_name)\n",
        "        if not os.path.exists(class_dir):\n",
        "            continue\n",
        "\n",
        "        print(f\"Loading {class_name} images...\")\n",
        "        for img_file in os.listdir(class_dir):\n",
        "            img_path = os.path.join(class_dir, img_file)\n",
        "            try:\n",
        "                img = cv2.imread(img_path)\n",
        "                if img is None:\n",
        "                    continue\n",
        "\n",
        "                img = cv2.resize(img, (224, 224))\n",
        "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "                img = img.astype(np.float32) / 255.0  # Normalize\n",
        "\n",
        "                X_test.append(img)\n",
        "                y_test.append(class_idx)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {img_path}: {str(e)}\")\n",
        "\n",
        "    return np.array(X_test), np.array(y_test)\n",
        "\n",
        "X_test, y_test = load_test_data(test_dir)\n",
        "print(f\"\\nLoaded {len(X_test)} test samples\")\n",
        "\n",
        "# Feature extraction\n",
        "def extract_hog_features(image):\n",
        "    if len(image.shape) > 2:\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "    if image.dtype != np.uint8:\n",
        "        image = (image * 255).astype(np.uint8)\n",
        "    features = hog(\n",
        "        image,\n",
        "        orientations=9,\n",
        "        pixels_per_cell=(8, 8),\n",
        "        cells_per_block=(2, 2),\n",
        "        block_norm='L2-Hys',\n",
        "        visualize=False,\n",
        "        feature_vector=True\n",
        "    )\n",
        "    return features\n",
        "\n",
        "# Evaluation functions\n",
        "# Evaluation functions\n",
        "def evaluate_individual_models(models, X_test, y_test):\n",
        "    results = {}\n",
        "    y_true = y_test\n",
        "\n",
        "    for model_name, model in models.items():\n",
        "        print(f\"\\nEvaluating {model_name}...\")\n",
        "        y_pred = []\n",
        "\n",
        "        print(f\"Predicting with {model_name}...\")\n",
        "        for img in X_test:\n",
        "            try:\n",
        "                if model_name in ['VGG19', 'DenseNet', 'CustomCNN', 'ResNet50']:\n",
        "                    img_reshaped = np.expand_dims(img, axis=0)\n",
        "                    pred = model.predict(img_reshaped, verbose=0)\n",
        "                    y_pred.append(np.argmax(pred))\n",
        "                elif model_name == 'RandomForest':\n",
        "                    features = extract_hog_features(img)\n",
        "                    pred = model.predict([features])\n",
        "                    y_pred.append(pred[0])\n",
        "                elif model_name == 'SVM':\n",
        "                    img_gray = cv2.cvtColor((img * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
        "                    # Apply the same resizing as during training\n",
        "                    img_resized = cv2.resize(img_gray, (224, 224))\n",
        "                    # Extract Gabor features (assuming you have this function defined or will define it)\n",
        "                    gabor_features = extract_gabor_features([img_resized])\n",
        "                    pred = model.predict(gabor_features)\n",
        "                    y_pred.append(pred[0])\n",
        "            except Exception as e:\n",
        "                print(f\"Prediction error with {model_name}: {str(e)}\")\n",
        "                y_pred.append(-1)  # Append a placeholder for error\n",
        "\n",
        "        # Filter out error predictions for evaluation\n",
        "        valid_preds = [p for p in y_pred if p != -1]\n",
        "        valid_true = [y_true[i] for i, p in enumerate(y_pred) if p != -1]\n",
        "\n",
        "        if valid_preds:\n",
        "            # Calculate accuracy and F1-score\n",
        "            acc = accuracy_score(valid_true, valid_preds)\n",
        "            f1 = f1_score(valid_true, valid_preds, average='weighted')\n",
        "            report = classification_report(valid_true, valid_preds, target_names=class_labels, zero_division=0)\n",
        "            results[model_name] = {\n",
        "                'accuracy': acc,\n",
        "                'f1': f1,\n",
        "                'y_pred': np.array(valid_preds),\n",
        "                'y_true': np.array(valid_true),\n",
        "                'report': report\n",
        "            }\n",
        "            print(f\"{model_name} Accuracy: {acc:.4f}, F1-score: {f1:.4f}\")\n",
        "        else:\n",
        "            print(f\"No valid predictions for {model_name}.\")\n",
        "            results[model_name] = {'accuracy': 0.0, 'f1': 0.0, 'y_pred': np.array([]), 'y_true': np.array([]), 'report': ''}\n",
        "\n",
        "    return results\n",
        "\n",
        "# You need to define the extract_gabor_features function here,\n",
        "# exactly as it was used during the SVM training.\n",
        "def extract_gabor_features(images):\n",
        "    features = []\n",
        "    kernels = []\n",
        "\n",
        "    for theta in range(4):\n",
        "        theta = theta / 4. * np.pi\n",
        "        for sigma in (1, 3):\n",
        "            for lamda in np.arange(0, np.pi, np.pi / 4):\n",
        "                for gamma in (0.05, 0.5):\n",
        "                    kernel = cv2.getGaborKernel((21, 21), sigma, theta, lamda, gamma, 0, ktype=cv2.CV_32F)\n",
        "                    kernels.append(kernel)\n",
        "\n",
        "    for image in images:\n",
        "        feature_vector = []\n",
        "        for kernel in kernels:\n",
        "            filtered = cv2.filter2D(image, cv2.CV_8UC3, kernel)\n",
        "            feature_vector.append(filtered.mean())\n",
        "        features.append(feature_vector)\n",
        "\n",
        "    return np.array(features)\n",
        "\n",
        "# Modify the stacking hybrid model as well to use Gabor features for SVM\n",
        "def evaluate_stacking_hybrid_model(models, top_model_names, X_test, y_test):\n",
        "    print(\"\\nEvaluating Stacking Hybrid Model (Top Models Only)...\")\n",
        "    meta_features = []\n",
        "\n",
        "    for img in X_test:\n",
        "        img_meta_features = []\n",
        "        for model_name in top_model_names:\n",
        "            if model_name in models:\n",
        "                model = models[model_name]\n",
        "                try:\n",
        "                    if model_name in ['VGG19', 'DenseNet', 'CustomCNN', 'ResNet50']:\n",
        "                        img_reshaped = np.expand_dims(img, axis=0)\n",
        "                        pred = model.predict(img_reshaped, verbose=0)\n",
        "                        img_meta_features.extend(pred[0])\n",
        "                    elif model_name == 'RandomForest':\n",
        "                        features = extract_hog_features(img)\n",
        "                        pred = model.predict_proba([features])\n",
        "                        img_meta_features.extend(pred[0])\n",
        "                    elif model_name == 'SVM':\n",
        "                        img_gray = cv2.cvtColor((img * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
        "                        img_resized = cv2.resize(img_gray, (224, 224))\n",
        "                        gabor_features = extract_gabor_features([img_resized])\n",
        "                        pred = model.predict_proba(gabor_features)\n",
        "                        img_meta_features.extend(pred[0])\n",
        "                except Exception as e:\n",
        "                    print(f\"Error creating meta-features for {model_name}: {str(e)}\")\n",
        "                    img_meta_features.extend([1.0/len(class_labels)]*len(class_labels))\n",
        "            else:\n",
        "                print(f\"Warning: Top model '{model_name}' not found in loaded models.\")\n",
        "                img_meta_features.extend([1.0/len(class_labels)]*len(class_labels))\n",
        "\n",
        "        meta_features.append(img_meta_features)\n",
        "\n",
        "    meta_features = np.array(meta_features)\n",
        "\n",
        "    # Train stacking classifier\n",
        "    stacking_model = LogisticRegression(max_iter=1000)\n",
        "    stacking_model.fit(meta_features, y_test)\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred = stacking_model.predict(meta_features)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    # Plot confusion matrix for the stacking model\n",
        "    plot_stacking_confusion_matrix(y_test, y_pred, model_name='Stacking Hybrid')\n",
        "\n",
        "    results = {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'y_pred': y_pred,\n",
        "        'y_true': y_test,\n",
        "        'report': classification_report(y_test, y_pred, target_names=class_labels, zero_division=0)\n",
        "    }\n",
        "    print(f\"Stacking Hybrid Model (Top) Accuracy: {acc:.4f}, F1-score: {f1:.4f}\")\n",
        "    return results\n",
        "\n",
        "# Main execution remains the same\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\nStarting evaluations...\")\n",
        "    individual_results = evaluate_individual_models(loaded_models, X_test, y_test)\n",
        "\n",
        "    # Prepare data for DataFrame, ensuring numeric values\n",
        "    results_data = {}\n",
        "    for name, res in individual_results.items():\n",
        "        if 'accuracy' in res and 'f1' in res:\n",
        "            results_data[name] = {'accuracy': res['accuracy'], 'f1': res['f1']}\n",
        "\n",
        "    results_df = pd.DataFrame(results_data).T\n",
        "\n",
        "    if not results_df.empty:\n",
        "        # Convert columns to numeric, coercing errors to NaN\n",
        "        results_df['accuracy'] = pd.to_numeric(results_df['accuracy'], errors='coerce')\n",
        "        results_df['f1'] = pd.to_numeric(results_df['f1'], errors='coerce')\n",
        "\n",
        "        # Calculate Combined Score, handling potential NaNs\n",
        "        results_df['Combined_Score'] = results_df[['accuracy', 'f1']].mean(axis=1)\n",
        "\n",
        "        # Drop rows with NaN in Combined_Score (if any model evaluation failed significantly)\n",
        "        results_df_valid = results_df.dropna(subset=['Combined_Score'])\n",
        "\n",
        "        if not results_df_valid.empty:\n",
        "            top_3_models = results_df_valid.nlargest(3, 'Combined_Score').index.tolist()\n",
        "            print(f\"\\nTop 3 Models considered for stacking: {top_3_models}\")\n",
        "        else:\n",
        "            top_3_models = []\n",
        "            print(\"\\nWarning: Not enough valid individual model results to determine top models.\")\n",
        "    else:\n",
        "        top_3_models = []\n",
        "        print(\"\\nWarning: No individual model results available to determine top models.\")\n",
        "\n",
        "    # ... rest of your if __name__ block remains the same, using the determined top_3_models ...\n",
        "\n",
        "    # Separate results for Deep Learning and Machine Learning models\n",
        "    dl_results = {k: v for k, v in individual_results.items() if k in ['VGG19', 'DenseNet', 'CustomCNN', 'ResNet50']}\n",
        "    ml_results = {k: v for k, v in individual_results.items() if k in ['RandomForest', 'SVM']}\n",
        "\n",
        "    # Plot Confusion Matrices for individual models\n",
        "    for model_name, result in individual_results.items():\n",
        "        if result['y_pred'].size > 0:\n",
        "            plot_confusion_matrix(result['y_true'], result['y_pred'], model_name)\n",
        "\n",
        "    # Plot comparison of individual models\n",
        "    plot_comparison(individual_results, \"Comparison of Individual Model Performance\", \"individual_model_comparison.png\")\n",
        "\n",
        "    # Plot grouped comparison of DL vs ML models\n",
        "    plot_comparison_grouped(dl_results, ml_results, \"Comparison of Deep Learning vs Machine Learning Models\", \"dl_ml_comparison.png\")\n",
        "\n",
        "    stacking_results = None\n",
        "    # Evaluate Hybrid Model if enough top models are available\n",
        "    if len(top_3_models) >= 2:\n",
        "        stacking_results = evaluate_stacking_hybrid_model(loaded_models, top_3_models, X_test, y_test)\n",
        "        print(f\"\\nStacking Hybrid Model (Top): Accuracy: {stacking_results['accuracy']:.4f}\")\n",
        "\n",
        "        comparison_data = individual_results.copy()\n",
        "        comparison_data['Stacking Hybrid '] = {\n",
        "            'accuracy': stacking_results['accuracy'],\n",
        "            'f1': stacking_results['f1']\n",
        "        }\n",
        "        plot_comparison(comparison_data, \"Comparison with Stacking Hybrid (Top) Model\", \"model_comparison_with_hybrid_top.png\")\n",
        "\n",
        "        if stacking_results and 'y_pred' in stacking_results and 'y_true' in stacking_results:\n",
        "            plot_confusion_matrix(stacking_results['y_true'], stacking_results['y_pred'], 'Stacking Hybrid (Top)')\n",
        "\n",
        "    else:\n",
        "        print(\"\\nNot enough top models available for stacking (need at least 2).\")\n",
        "        comparison_data = individual_results.copy()\n",
        "\n",
        "    # Single graph comparison of all models (individual + hybrid)\n",
        "    comparison_data['Stacking Hybrid (All)'] = comparison_data.get('Stacking Hybrid', {'accuracy': np.nan, 'f1': np.nan}) # Keep the old one if it ran\n",
        "    if stacking_results:\n",
        "        comparison_data['Stacking Hybrid'] = stacking_results\n",
        "    plot_comparison(comparison_data, \"Comparison of All Models (Individual & Hybrid)\", \"all_models_comparison.png\")\n",
        "\n",
        "    # Plot training history for individual models\n",
        "    for model_name in loaded_models.keys():\n",
        "        if model_name in ['VGG19', 'DenseNet', 'CustomCNN', 'ResNet50']:\n",
        "            plot_training_history(model_name)\n",
        "\n",
        "    print(\"\\n\\n=== Evaluation Summary ===\")\n",
        "    for model_name in loaded_models.keys():\n",
        "        if model_name in individual_results:\n",
        "            print(f\"{model_name}: Accuracy: {individual_results[model_name]['accuracy']:.4f}, F1-Score: {individual_results[model_name]['f1']:.4f}\")\n",
        "    if stacking_results:\n",
        "        print(f\"\\nStacking Hybrid Model (Top): Accuracy: {stacking_results['accuracy']:.4f}, F1-Score: {stacking_results['f1']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN vs Random Forest performance plot saved to cnn_rf_comparison.png\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_cnn_rf_comparison(cnn_results, rf_results, output_path='cnn_rf_comparison.png'):\n",
        "    \"\"\"\n",
        "    Generates a bar graph comparing the performance metrics (Accuracy and F1-Score)\n",
        "    of a Custom CNN and a Random Forest model.\n",
        "\n",
        "    Args:\n",
        "        cnn_results (dict): Dictionary containing the evaluation results of the CNN.\n",
        "                             Expected keys: 'accuracy', 'f1'.\n",
        "        rf_results (dict): Dictionary containing the evaluation results of the Random Forest.\n",
        "                            Expected keys: 'accuracy', 'f1'.\n",
        "        output_path (str, optional): Path to save the generated plot.\n",
        "                                     Defaults to 'cnn_rf_comparison.png'.\n",
        "    \"\"\"\n",
        "    model_names = ['CNN', 'Random Forest']\n",
        "    accuracy_scores = [cnn_results.get('accuracy', np.nan), rf_results.get('accuracy', np.nan)]\n",
        "    f1_scores = [cnn_results.get('f1', np.nan), rf_results.get('f1', np.nan)]\n",
        "\n",
        "    x = np.arange(len(model_names))\n",
        "    width = 0.35\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    rects1 = ax.bar(x - width/2, accuracy_scores, width, label='Accuracy')\n",
        "    rects2 = ax.bar(x + width/2, f1_scores, width, label='F1-Score')\n",
        "\n",
        "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
        "    ax.set_ylabel('Score')\n",
        "    ax.set_title('Comparison of Custom CNN and Random Forest Performance')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(model_names)\n",
        "    ax.legend()\n",
        "    ax.set_ylim(0, 1.05) # Set y-axis limit for better visualization of scores (0 to 1)\n",
        "\n",
        "    def autolabel(rects):\n",
        "        \"\"\"Attach a text label above each bar in *rects*, indicating its height.\"\"\"\n",
        "        for rect in rects:\n",
        "            height = rect.get_height()\n",
        "            if not np.isnan(height):\n",
        "                ax.annotate(f'{height:.4f}',\n",
        "                            xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                            xytext=(0, 3),  # 3 points vertical offset\n",
        "                            textcoords=\"offset points\",\n",
        "                            ha='center', va='bottom')\n",
        "\n",
        "    autolabel(rects1)\n",
        "    autolabel(rects2)\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.savefig(output_path)\n",
        "    plt.show()\n",
        "\n",
        "# Example usage (assuming you have the 'individual_results' dictionary):\n",
        "if __name__ == \"__main__\":\n",
        "    # Assume 'individual_results' dictionary is populated after evaluating models\n",
        "    individual_results = {\n",
        "        'VGG19': {'accuracy': 0.85, 'f1': 0.83},\n",
        "        'DenseNet': {'accuracy': 0.92, 'f1': 0.91},\n",
        "        'CustomCNN': {'accuracy': 0.9603, 'f1': 0.9600},\n",
        "        'ResNet50': {'accuracy': 0.88, 'f1': 0.87},\n",
        "        'RandomForest': {'accuracy': 0.8940, 'f1': 0.8924},\n",
        "        'SVM': {'accuracy': 0.78, 'f1': 0.75}\n",
        "    }\n",
        "\n",
        "    if 'CustomCNN' in individual_results and 'RandomForest' in individual_results:\n",
        "        cnn_results = individual_results['CustomCNN']\n",
        "        rf_results = individual_results['RandomForest']\n",
        "        plot_cnn_rf_comparison(cnn_results, rf_results, output_path='cnn_rf_comparison.png')\n",
        "        print(\"CNN vs Random Forest performance plot saved to cnn_rf_comparison.png\")\n",
        "    else:\n",
        "        print(\"Could not find results for both Custom CNN and Random Forest in individual_results.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cerebralFusion_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
